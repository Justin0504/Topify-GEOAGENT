"""
title: æŠ€æœ¯SEOå®¡è®¡å·¥å…·
description: ã€æŠ€æœ¯SEOæ£€æŸ¥ã€‘æ‰«æç½‘ç«™æŠ€æœ¯SEOé—®é¢˜ï¼ŒåŒºåˆ†éœ€ä¿®å¤å’Œéœ€ä¼˜åŒ–çš„é—®é¢˜ï¼Œå¹¶ç»™å‡ºå…·ä½“å»ºè®®
author: GEO Agent
version: 1.0.0
required_open_webui_version: 0.6.0
requirements: openpyxl, requests, beautifulsoup4
"""

import os
import requests
from typing import Dict, Any, List, Optional
from datetime import datetime
from pydantic import BaseModel, Field
from urllib.parse import urljoin, urlparse

try:
    from bs4 import BeautifulSoup
    BS4_AVAILABLE = True
except ImportError:
    BS4_AVAILABLE = False

from openpyxl import Workbook
from openpyxl.styles import Font, Alignment, PatternFill, Border, Side
from openpyxl.utils import get_column_letter


class Tools:
    """
    æŠ€æœ¯SEOå®¡è®¡å·¥å…· - æ‰«æç½‘ç«™æŠ€æœ¯é—®é¢˜å¹¶ç”Ÿæˆå®¡è®¡æŠ¥å‘Š
    
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    ğŸ¯ åŠŸèƒ½åŒ¹é…æŒ‡å—ï¼ˆä¸­æ–‡è§¦å‘è¯ï¼‰
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    ğŸ”§ technical_seo_audit - æŠ€æœ¯SEOå®¡è®¡
       è§¦å‘è¯: "æŠ€æœ¯SEO", "SEOæ£€æŸ¥", "ç½‘ç«™å®¡è®¡", "SEOå®¡è®¡",
              "æŠ€æœ¯é—®é¢˜", "SEOé—®é¢˜æ‰«æ", "ç½‘ç«™è¯Šæ–­"
       ç¤ºä¾‹: "å¯¹ example.com è¿›è¡ŒæŠ€æœ¯SEOæ£€æŸ¥"
       è¾“å‡º: Excel æ–‡ä»¶ï¼ˆé—®é¢˜åˆ—è¡¨å’Œä¿®å¤å»ºè®®ï¼‰
    
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """

    class Valves(BaseModel):
        OUTPUT_PATH: str = Field(
            default="/app/backend/data/output",
            description="æ–‡ä»¶ä¿å­˜è·¯å¾„ï¼ˆDockerç¯å¢ƒï¼‰"
        )
        REQUEST_TIMEOUT: int = Field(
            default=10,
            description="HTTPè¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰"
        )

    def __init__(self):
        self.valves = self.Valves()

    def _ts_ensure_output_dir(self) -> str:
        """ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨"""
        output_path = self.valves.OUTPUT_PATH
        if not os.path.exists(output_path):
            os.makedirs(output_path, exist_ok=True)
        return output_path

    def _ts_style_excel_sheet(self, ws, headers: List[str], header_color: str = "4472C4"):
        """ä¸ºExcelè¡¨æ ¼æ·»åŠ æ ·å¼"""
        header_fill = PatternFill(start_color=header_color, end_color=header_color, fill_type="solid")
        header_font = Font(color="FFFFFF", bold=True, size=11)
        border = Border(
            left=Side(style='thin'),
            right=Side(style='thin'),
            top=Side(style='thin'),
            bottom=Side(style='thin')
        )
        
        for col, header in enumerate(headers, 1):
            cell = ws.cell(row=1, column=col, value=header)
            cell.fill = header_fill
            cell.font = header_font
            cell.alignment = Alignment(horizontal='center', vertical='center', wrap_text=True)
            cell.border = border
        
        ws.freeze_panes = 'A2'

    def _fetch_page(self, url: str) -> dict:
        """è·å–é¡µé¢å†…å®¹"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (compatible; SEOBot/1.0; +http://example.com/bot)'
            }
            response = requests.get(url, headers=headers, timeout=self.valves.REQUEST_TIMEOUT, allow_redirects=True)
            return {
                "success": True,
                "status_code": response.status_code,
                "content": response.text,
                "headers": dict(response.headers),
                "url": response.url,
                "elapsed": response.elapsed.total_seconds()
            }
        except requests.exceptions.Timeout:
            return {"success": False, "error": "è¯·æ±‚è¶…æ—¶"}
        except requests.exceptions.RequestException as e:
            return {"success": False, "error": str(e)}

    def _analyze_page(self, url: str, html_content: str, headers: dict) -> List[dict]:
        """åˆ†æé¡µé¢SEOé—®é¢˜"""
        issues = []
        
        if not BS4_AVAILABLE:
            issues.append({
                "category": "ç³»ç»Ÿ",
                "severity": "è­¦å‘Š",
                "issue": "BeautifulSoupæœªå®‰è£…",
                "description": "æ— æ³•è¿›è¡Œæ·±åº¦HTMLåˆ†æ",
                "recommendation": "å®‰è£… beautifulsoup4: pip install beautifulsoup4",
                "impact": "ä¸­"
            })
            return issues
        
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # 1. æ£€æŸ¥ Title æ ‡ç­¾
        title = soup.find('title')
        if not title:
            issues.append({
                "category": "Metaæ ‡ç­¾",
                "severity": "ä¸¥é‡",
                "issue": "ç¼ºå°‘Titleæ ‡ç­¾",
                "description": "é¡µé¢æ²¡æœ‰<title>æ ‡ç­¾",
                "recommendation": "æ·»åŠ å”¯ä¸€ã€æè¿°æ€§çš„titleæ ‡ç­¾ï¼ˆ50-60å­—ç¬¦ï¼‰",
                "impact": "é«˜"
            })
        elif title:
            title_text = title.get_text().strip()
            if len(title_text) < 30:
                issues.append({
                    "category": "Metaæ ‡ç­¾",
                    "severity": "è­¦å‘Š",
                    "issue": "Titleè¿‡çŸ­",
                    "description": f"Titleä»…{len(title_text)}å­—ç¬¦: '{title_text}'",
                    "recommendation": "Titleå»ºè®®50-60å­—ç¬¦ï¼ŒåŒ…å«ä¸»è¦å…³é”®è¯",
                    "impact": "ä¸­"
                })
            elif len(title_text) > 60:
                issues.append({
                    "category": "Metaæ ‡ç­¾",
                    "severity": "å»ºè®®",
                    "issue": "Titleè¿‡é•¿",
                    "description": f"Titleæœ‰{len(title_text)}å­—ç¬¦ï¼Œå¯èƒ½è¢«æˆªæ–­",
                    "recommendation": "Titleå»ºè®®50-60å­—ç¬¦",
                    "impact": "ä½"
                })
        
        # 2. æ£€æŸ¥ Meta Description
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if not meta_desc:
            issues.append({
                "category": "Metaæ ‡ç­¾",
                "severity": "ä¸¥é‡",
                "issue": "ç¼ºå°‘Meta Description",
                "description": "é¡µé¢æ²¡æœ‰meta descriptionæ ‡ç­¾",
                "recommendation": "æ·»åŠ æè¿°æ€§çš„meta descriptionï¼ˆ150-160å­—ç¬¦ï¼‰",
                "impact": "é«˜"
            })
        elif meta_desc:
            desc_content = meta_desc.get('content', '').strip()
            if len(desc_content) < 70:
                issues.append({
                    "category": "Metaæ ‡ç­¾",
                    "severity": "è­¦å‘Š",
                    "issue": "Meta Descriptionè¿‡çŸ­",
                    "description": f"æè¿°ä»…{len(desc_content)}å­—ç¬¦",
                    "recommendation": "å»ºè®®150-160å­—ç¬¦ï¼ŒåŒ…å«å…³é”®è¯å’Œå·å¬æ€§ç”¨è¯­",
                    "impact": "ä¸­"
                })
            elif len(desc_content) > 160:
                issues.append({
                    "category": "Metaæ ‡ç­¾",
                    "severity": "å»ºè®®",
                    "issue": "Meta Descriptionè¿‡é•¿",
                    "description": f"æè¿°æœ‰{len(desc_content)}å­—ç¬¦ï¼Œå¯èƒ½è¢«æˆªæ–­",
                    "recommendation": "å»ºè®®150-160å­—ç¬¦",
                    "impact": "ä½"
                })
        
        # 3. æ£€æŸ¥ H1 æ ‡ç­¾
        h1_tags = soup.find_all('h1')
        if len(h1_tags) == 0:
            issues.append({
                "category": "æ ‡é¢˜ç»“æ„",
                "severity": "ä¸¥é‡",
                "issue": "ç¼ºå°‘H1æ ‡ç­¾",
                "description": "é¡µé¢æ²¡æœ‰H1æ ‡ç­¾",
                "recommendation": "æ¯ä¸ªé¡µé¢åº”æœ‰ä¸”ä»…æœ‰ä¸€ä¸ªH1æ ‡ç­¾",
                "impact": "é«˜"
            })
        elif len(h1_tags) > 1:
            issues.append({
                "category": "æ ‡é¢˜ç»“æ„",
                "severity": "è­¦å‘Š",
                "issue": "å¤šä¸ªH1æ ‡ç­¾",
                "description": f"é¡µé¢æœ‰{len(h1_tags)}ä¸ªH1æ ‡ç­¾",
                "recommendation": "æ¯ä¸ªé¡µé¢åº”ä»…æœ‰ä¸€ä¸ªH1æ ‡ç­¾",
                "impact": "ä¸­"
            })
        
        # 4. æ£€æŸ¥å›¾ç‰‡ Alt å±æ€§
        images = soup.find_all('img')
        images_without_alt = [img for img in images if not img.get('alt')]
        if images_without_alt:
            issues.append({
                "category": "å›¾ç‰‡ä¼˜åŒ–",
                "severity": "è­¦å‘Š",
                "issue": "å›¾ç‰‡ç¼ºå°‘Altå±æ€§",
                "description": f"{len(images_without_alt)}/{len(images)}å¼ å›¾ç‰‡ç¼ºå°‘altå±æ€§",
                "recommendation": "ä¸ºæ‰€æœ‰å›¾ç‰‡æ·»åŠ æè¿°æ€§çš„altæ–‡æœ¬",
                "impact": "ä¸­"
            })
        
        # 5. æ£€æŸ¥ Canonical æ ‡ç­¾
        canonical = soup.find('link', attrs={'rel': 'canonical'})
        if not canonical:
            issues.append({
                "category": "æŠ€æœ¯SEO",
                "severity": "è­¦å‘Š",
                "issue": "ç¼ºå°‘Canonicalæ ‡ç­¾",
                "description": "é¡µé¢æ²¡æœ‰canonicalæ ‡ç­¾",
                "recommendation": "æ·»åŠ canonicalæ ‡ç­¾é¿å…é‡å¤å†…å®¹é—®é¢˜",
                "impact": "ä¸­"
            })
        
        # 6. æ£€æŸ¥ Robots Meta
        robots_meta = soup.find('meta', attrs={'name': 'robots'})
        if robots_meta:
            content = robots_meta.get('content', '').lower()
            if 'noindex' in content:
                issues.append({
                    "category": "ç´¢å¼•æ§åˆ¶",
                    "severity": "ä¸¥é‡",
                    "issue": "é¡µé¢è®¾ç½®ä¸ºnoindex",
                    "description": "é¡µé¢è¢«è®¾ç½®ä¸ºä¸ç´¢å¼•",
                    "recommendation": "å¦‚éæ•…æ„ï¼Œç§»é™¤noindexæŒ‡ä»¤",
                    "impact": "é«˜"
                })
        
        # 7. æ£€æŸ¥ç§»åŠ¨ç«¯viewport
        viewport = soup.find('meta', attrs={'name': 'viewport'})
        if not viewport:
            issues.append({
                "category": "ç§»åŠ¨ç«¯ä¼˜åŒ–",
                "severity": "ä¸¥é‡",
                "issue": "ç¼ºå°‘Viewport metaæ ‡ç­¾",
                "description": "é¡µé¢æ²¡æœ‰è®¾ç½®viewport",
                "recommendation": "æ·»åŠ : <meta name='viewport' content='width=device-width, initial-scale=1'>",
                "impact": "é«˜"
            })
        
        # 8. æ£€æŸ¥ HTTPS
        if not url.startswith('https://'):
            issues.append({
                "category": "å®‰å…¨æ€§",
                "severity": "ä¸¥é‡",
                "issue": "æœªä½¿ç”¨HTTPS",
                "description": "ç½‘ç«™æœªä½¿ç”¨HTTPSåŠ å¯†",
                "recommendation": "é…ç½®SSLè¯ä¹¦ï¼Œå¯ç”¨HTTPS",
                "impact": "é«˜"
            })
        
        # 9. æ£€æŸ¥ç»“æ„åŒ–æ•°æ®
        schema_scripts = soup.find_all('script', attrs={'type': 'application/ld+json'})
        if not schema_scripts:
            issues.append({
                "category": "ç»“æ„åŒ–æ•°æ®",
                "severity": "å»ºè®®",
                "issue": "ç¼ºå°‘ç»“æ„åŒ–æ•°æ®",
                "description": "é¡µé¢æ²¡æœ‰Schema.orgç»“æ„åŒ–æ•°æ®",
                "recommendation": "æ·»åŠ é€‚åˆå†…å®¹ç±»å‹çš„JSON-LDç»“æ„åŒ–æ•°æ®",
                "impact": "ä¸­"
            })
        
        # 10. æ£€æŸ¥å†…éƒ¨é“¾æ¥
        internal_links = soup.find_all('a', href=True)
        broken_links = []
        for link in internal_links[:10]:  # åªæ£€æŸ¥å‰10ä¸ªé“¾æ¥
            href = link.get('href', '')
            if href.startswith('/') or url in href:
                # è¿™æ˜¯å†…éƒ¨é“¾æ¥ï¼Œå¯ä»¥è¿›ä¸€æ­¥æ£€æŸ¥
                pass
        
        # 11. æ£€æŸ¥é¡µé¢åŠ è½½ç›¸å…³
        if headers:
            # æ£€æŸ¥å‹ç¼©
            if 'gzip' not in headers.get('Content-Encoding', '').lower():
                issues.append({
                    "category": "æ€§èƒ½ä¼˜åŒ–",
                    "severity": "å»ºè®®",
                    "issue": "æœªå¯ç”¨Gzipå‹ç¼©",
                    "description": "é¡µé¢å“åº”æœªä½¿ç”¨gzipå‹ç¼©",
                    "recommendation": "åœ¨æœåŠ¡å™¨é…ç½®ä¸­å¯ç”¨gzipå‹ç¼©",
                    "impact": "ä¸­"
                })
            
            # æ£€æŸ¥ç¼“å­˜
            cache_control = headers.get('Cache-Control', '')
            if not cache_control:
                issues.append({
                    "category": "æ€§èƒ½ä¼˜åŒ–",
                    "severity": "å»ºè®®",
                    "issue": "æœªè®¾ç½®ç¼“å­˜ç­–ç•¥",
                    "description": "å“åº”å¤´æ²¡æœ‰Cache-Control",
                    "recommendation": "è®¾ç½®é€‚å½“çš„ç¼“å­˜ç­–ç•¥",
                    "impact": "ä½"
                })
        
        return issues

    def technical_seo_audit(
        self,
        domain: str,
        pages_to_check: str = ""
    ) -> str:
        """
        ã€æŠ€æœ¯SEOå®¡è®¡å·¥å…·ã€‘æ‰«æç½‘ç«™æŠ€æœ¯SEOé—®é¢˜ï¼Œç”Ÿæˆè¯¦ç»†å®¡è®¡æŠ¥å‘Š
        
        å½“ç”¨æˆ·è¯´ä»¥ä¸‹å†…å®¹æ—¶è°ƒç”¨æ­¤å·¥å…·ï¼š
        - "æŠ€æœ¯SEOæ£€æŸ¥"ã€"SEOå®¡è®¡"ã€"ç½‘ç«™è¯Šæ–­"
        - "æ‰«æSEOé—®é¢˜"ã€"æŠ€æœ¯é—®é¢˜æ£€æŸ¥"
        - "ç½‘ç«™SEOå¥åº·æ£€æŸ¥"
        
        :param domain: è¦æ£€æŸ¥çš„ç½‘ç«™åŸŸåï¼ˆå¦‚ example.comï¼‰
        :param pages_to_check: è¦é¢å¤–æ£€æŸ¥çš„é¡µé¢è·¯å¾„ï¼ˆé€—å·åˆ†éš”ï¼Œå¦‚ "/about,/contact"ï¼‰
        :return: åŒ…å«Excelå®¡è®¡æŠ¥å‘Šè·¯å¾„çš„ç»“æœ
        """
        output_path = self._ts_ensure_output_dir()
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"technical_seo_audit_{domain.replace('.', '_')}_{timestamp}.xlsx"
        filepath = os.path.join(output_path, filename)
        
        # æ„å»ºURLåˆ—è¡¨
        base_url = f"https://{domain}" if not domain.startswith('http') else domain
        urls_to_check = [base_url]
        
        if pages_to_check:
            for page in pages_to_check.split(','):
                page = page.strip()
                if page:
                    urls_to_check.append(urljoin(base_url, page))
        
        wb = Workbook()
        all_issues = []
        page_results = []
        
        # æ£€æŸ¥æ¯ä¸ªé¡µé¢
        for url in urls_to_check:
            result = self._fetch_page(url)
            
            if result["success"]:
                issues = self._analyze_page(url, result["content"], result.get("headers", {}))
                for issue in issues:
                    issue["page"] = url
                all_issues.extend(issues)
                
                page_results.append({
                    "url": url,
                    "status": result["status_code"],
                    "load_time": f"{result['elapsed']:.2f}s",
                    "issues_count": len(issues)
                })
            else:
                all_issues.append({
                    "category": "å¯è®¿é—®æ€§",
                    "severity": "ä¸¥é‡",
                    "issue": "é¡µé¢æ— æ³•è®¿é—®",
                    "description": result.get("error", "æœªçŸ¥é”™è¯¯"),
                    "recommendation": "æ£€æŸ¥URLæ˜¯å¦æ­£ç¡®ï¼ŒæœåŠ¡å™¨æ˜¯å¦æ­£å¸¸è¿è¡Œ",
                    "impact": "é«˜",
                    "page": url
                })
                page_results.append({
                    "url": url,
                    "status": "é”™è¯¯",
                    "load_time": "-",
                    "issues_count": 1
                })
        
        # ===== Sheet 1: é—®é¢˜æ±‡æ€» =====
        ws1 = wb.active
        ws1.title = "é—®é¢˜æ±‡æ€»"
        
        issue_headers = ["é—®é¢˜ID", "é¡µé¢", "ç±»åˆ«", "ä¸¥é‡ç¨‹åº¦", "é—®é¢˜", "æè¿°", "ä¿®å¤å»ºè®®", "å½±å“ç¨‹åº¦"]
        self._ts_style_excel_sheet(ws1, issue_headers, "C00000")
        
        # æŒ‰ä¸¥é‡ç¨‹åº¦æ’åº
        severity_order = {"ä¸¥é‡": 0, "è­¦å‘Š": 1, "å»ºè®®": 2}
        sorted_issues = sorted(all_issues, key=lambda x: severity_order.get(x.get("severity", "å»ºè®®"), 3))
        
        for row_idx, issue in enumerate(sorted_issues, 2):
            ws1.cell(row=row_idx, column=1, value=f"SEO-{str(row_idx-1).zfill(3)}")
            ws1.cell(row=row_idx, column=2, value=issue.get("page", ""))
            ws1.cell(row=row_idx, column=3, value=issue.get("category", ""))
            ws1.cell(row=row_idx, column=4, value=issue.get("severity", ""))
            ws1.cell(row=row_idx, column=5, value=issue.get("issue", ""))
            ws1.cell(row=row_idx, column=6, value=issue.get("description", ""))
            ws1.cell(row=row_idx, column=7, value=issue.get("recommendation", ""))
            ws1.cell(row=row_idx, column=8, value=issue.get("impact", ""))
            
            # æ ¹æ®ä¸¥é‡ç¨‹åº¦ç€è‰²
            severity = issue.get("severity", "")
            if severity == "ä¸¥é‡":
                ws1.cell(row=row_idx, column=4).fill = PatternFill(start_color="FF0000", end_color="FF0000", fill_type="solid")
                ws1.cell(row=row_idx, column=4).font = Font(color="FFFFFF", bold=True)
            elif severity == "è­¦å‘Š":
                ws1.cell(row=row_idx, column=4).fill = PatternFill(start_color="FFA500", end_color="FFA500", fill_type="solid")
            elif severity == "å»ºè®®":
                ws1.cell(row=row_idx, column=4).fill = PatternFill(start_color="FFFF00", end_color="FFFF00", fill_type="solid")
        
        # è°ƒæ•´åˆ—å®½
        ws1.column_dimensions['A'].width = 12
        ws1.column_dimensions['B'].width = 40
        ws1.column_dimensions['C'].width = 15
        ws1.column_dimensions['D'].width = 12
        ws1.column_dimensions['E'].width = 25
        ws1.column_dimensions['F'].width = 40
        ws1.column_dimensions['G'].width = 50
        ws1.column_dimensions['H'].width = 12
        
        # ===== Sheet 2: é¡µé¢æ£€æŸ¥ç»“æœ =====
        ws2 = wb.create_sheet("é¡µé¢æ£€æŸ¥ç»“æœ")
        
        page_headers = ["é¡µé¢URL", "HTTPçŠ¶æ€", "åŠ è½½æ—¶é—´", "é—®é¢˜æ•°é‡"]
        self._ts_style_excel_sheet(ws2, page_headers, "2E75B6")
        
        for row_idx, page in enumerate(page_results, 2):
            ws2.cell(row=row_idx, column=1, value=page["url"])
            ws2.cell(row=row_idx, column=2, value=page["status"])
            ws2.cell(row=row_idx, column=3, value=page["load_time"])
            ws2.cell(row=row_idx, column=4, value=page["issues_count"])
        
        ws2.column_dimensions['A'].width = 50
        
        # ===== Sheet 3: ä¿®å¤ä¼˜å…ˆçº§ =====
        ws3 = wb.create_sheet("ä¿®å¤ä¼˜å…ˆçº§")
        
        priority_headers = ["ä¼˜å…ˆçº§", "ç±»åˆ«", "é—®é¢˜æ•°é‡", "å»ºè®®æ“ä½œ"]
        self._ts_style_excel_sheet(ws3, priority_headers, "70AD47")
        
        # ç»Ÿè®¡å„ç±»é—®é¢˜
        category_counts = {}
        for issue in all_issues:
            cat = issue.get("category", "å…¶ä»–")
            if cat not in category_counts:
                category_counts[cat] = {"ä¸¥é‡": 0, "è­¦å‘Š": 0, "å»ºè®®": 0}
            category_counts[cat][issue.get("severity", "å»ºè®®")] += 1
        
        row_idx = 2
        
        # å…ˆå¤„ç†ä¸¥é‡é—®é¢˜
        for cat, counts in category_counts.items():
            if counts["ä¸¥é‡"] > 0:
                ws3.cell(row=row_idx, column=1, value="ç«‹å³ä¿®å¤")
                ws3.cell(row=row_idx, column=2, value=cat)
                ws3.cell(row=row_idx, column=3, value=counts["ä¸¥é‡"])
                ws3.cell(row=row_idx, column=4, value="è¿™äº›é—®é¢˜ä¸¥é‡å½±å“SEOï¼Œéœ€è¦ç«‹å³å¤„ç†")
                ws3.cell(row=row_idx, column=1).fill = PatternFill(start_color="FF0000", end_color="FF0000", fill_type="solid")
                ws3.cell(row=row_idx, column=1).font = Font(color="FFFFFF", bold=True)
                row_idx += 1
        
        # å†å¤„ç†è­¦å‘Šé—®é¢˜
        for cat, counts in category_counts.items():
            if counts["è­¦å‘Š"] > 0:
                ws3.cell(row=row_idx, column=1, value="å°½å¿«ä¼˜åŒ–")
                ws3.cell(row=row_idx, column=2, value=cat)
                ws3.cell(row=row_idx, column=3, value=counts["è­¦å‘Š"])
                ws3.cell(row=row_idx, column=4, value="è¿™äº›é—®é¢˜å½±å“SEOæ•ˆæœï¼Œå»ºè®®åœ¨2å‘¨å†…å¤„ç†")
                ws3.cell(row=row_idx, column=1).fill = PatternFill(start_color="FFA500", end_color="FFA500", fill_type="solid")
                row_idx += 1
        
        # æœ€åå¤„ç†å»ºè®®
        for cat, counts in category_counts.items():
            if counts["å»ºè®®"] > 0:
                ws3.cell(row=row_idx, column=1, value="æŒç»­æ”¹è¿›")
                ws3.cell(row=row_idx, column=2, value=cat)
                ws3.cell(row=row_idx, column=3, value=counts["å»ºè®®"])
                ws3.cell(row=row_idx, column=4, value="è¿™äº›æ˜¯ä¼˜åŒ–å»ºè®®ï¼Œå¯ä»¥åœ¨æ—¥å¸¸ç»´æŠ¤ä¸­å¤„ç†")
                ws3.cell(row=row_idx, column=1).fill = PatternFill(start_color="FFFF00", end_color="FFFF00", fill_type="solid")
                row_idx += 1
        
        wb.save(filepath)
        
        # ç»Ÿè®¡
        critical_count = len([i for i in all_issues if i.get("severity") == "ä¸¥é‡"])
        warning_count = len([i for i in all_issues if i.get("severity") == "è­¦å‘Š"])
        suggestion_count = len([i for i in all_issues if i.get("severity") == "å»ºè®®"])
        
        return f"""
ğŸ“Š **æŠ€æœ¯SEOå®¡è®¡å®Œæˆ**

ğŸŒ ç½‘ç«™: {domain}
ğŸ“„ æ£€æŸ¥é¡µé¢æ•°: {len(urls_to_check)}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“ˆ **é—®é¢˜ç»Ÿè®¡**
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ”´ ä¸¥é‡é—®é¢˜: {critical_count} ä¸ªï¼ˆéœ€ç«‹å³ä¿®å¤ï¼‰
ğŸŸ  è­¦å‘Šé—®é¢˜: {warning_count} ä¸ªï¼ˆå»ºè®®2å‘¨å†…ä¼˜åŒ–ï¼‰
ğŸŸ¡ æ”¹è¿›å»ºè®®: {suggestion_count} ä¸ªï¼ˆæŒç»­ä¼˜åŒ–ï¼‰

ğŸ“‹ **æ€»è®¡**: {len(all_issues)} ä¸ªé—®é¢˜

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ’¾ **æ–‡ä»¶å·²ä¿å­˜**
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
è·¯å¾„: {filepath}

ğŸ“‹ åŒ…å«ä»¥ä¸‹å·¥ä½œè¡¨:
1. é—®é¢˜æ±‡æ€» - æ‰€æœ‰é—®é¢˜è¯¦æƒ…åŠä¿®å¤å»ºè®®
2. é¡µé¢æ£€æŸ¥ç»“æœ - å„é¡µé¢çŠ¶æ€å’ŒåŠ è½½æ—¶é—´
3. ä¿®å¤ä¼˜å…ˆçº§ - æŒ‰ä¼˜å…ˆçº§åˆ†ç±»çš„è¡ŒåŠ¨å»ºè®®

ğŸ’¡ **ä¼˜åŒ–å»ºè®®**:
1. ä¼˜å…ˆå¤„ç†"ä¸¥é‡"é—®é¢˜ï¼ˆçº¢è‰²æ ‡è®°ï¼‰
2. é‡ç‚¹å…³æ³¨ Metaæ ‡ç­¾ã€æ ‡é¢˜ç»“æ„ã€ç§»åŠ¨ç«¯ä¼˜åŒ–
3. å®šæœŸè¿›è¡ŒæŠ€æœ¯SEOæ£€æŸ¥ï¼ˆå»ºè®®æ¯æœˆä¸€æ¬¡ï¼‰
"""

