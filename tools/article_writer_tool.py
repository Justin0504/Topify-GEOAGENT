"""
title: æ–‡ç« å†™ä½œå·¥å…·
description: ã€å•ç¯‡æ–‡ç« ç”Ÿæˆã€‘æ ¹æ®æœç´¢æ„å›¾å†™ä½œGEOä¼˜åŒ–æ–‡ç«  | ã€å®Œæ•´æ–‡ç« ç”Ÿæˆã€‘å€’é‡‘å­—å¡”ç»“æ„å®Œæ•´æ–‡ç« ï¼ˆå«é…å›¾ã€é“¾æ¥ï¼‰ | ã€æ‰¹é‡æ–‡ç« ç”Ÿæˆã€‘ä¸ºå¤šä¸ªä¸»é¢˜æ‰¹é‡ç”Ÿæˆæ–‡ç« 
author: GEO Agent
version: 2.0.0
required_open_webui_version: 0.6.0
requirements: python-docx, requests, beautifulsoup4
"""

import os
import re
import requests
import urllib3
from typing import List, Optional
from datetime import datetime
from pydantic import BaseModel, Field
from urllib.parse import urlparse, urljoin
from io import BytesIO

from docx import Document
from docx.shared import Pt, RGBColor, Inches
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.oxml.ns import qn
from docx.oxml import OxmlElement

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)


class Tools:
    """
    æ–‡ç« å†™ä½œå·¥å…· - ç”ŸæˆSEO/GEOä¼˜åŒ–çš„æ–‡ç« å†…å®¹
    
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    ğŸ¯ åŠŸèƒ½åŒ¹é…æŒ‡å—ï¼ˆä¸­æ–‡è§¦å‘è¯ï¼‰
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    âœï¸ write_single_article - å•ç¯‡æ–‡ç« æ¡†æ¶ç”Ÿæˆ
       è§¦å‘è¯: "å†™æ–‡ç« æ¡†æ¶", "ç”Ÿæˆæ–‡ç« å¤§çº²", "åˆ›å»ºæ–‡ç« ç»“æ„",
              "æ–‡ç« æ¡†æ¶", "GEOæ–‡ç« æ¡†æ¶"
       ç¤ºä¾‹: "ä¸ºå…³é”®è¯ best AI tools ç”Ÿæˆæ–‡ç« æ¡†æ¶"
       è¾“å‡º: Word æ–‡æ¡£ï¼ˆåŒ…å«GEOä¼˜åŒ–ç»“æ„æ¡†æ¶ï¼‰
    
    ğŸ“ write_complete_article - å®Œæ•´æ–‡ç« ç”Ÿæˆï¼ˆæ”¯æŒWordPresså‘å¸ƒï¼‰
       è§¦å‘è¯: "å†™å®Œæ•´æ–‡ç« ", "ç”Ÿæˆå®Œæ•´æ–‡ç« ", "å€’é‡‘å­—å¡”æ–‡ç« ",
              "åˆ†ææœç´¢æ„å›¾å¹¶å†™æ–‡ç« ", "å†™ä¸€ç¯‡å®Œæ•´çš„æ–‡ç« ",
              "å†™æ–‡ç« å¹¶å‘å¸ƒ", "å‘å¸ƒåˆ°WordPress"
       ä½¿ç”¨æµç¨‹:
        1. LLMå…ˆæ ¹æ®æœç´¢å…³é”®è¯ã€äº§å“ä¿¡æ¯ç”Ÿæˆå®Œæ•´çš„æ–‡ç« å†…å®¹ï¼ˆMarkdownæ ¼å¼ï¼‰
        2. ç„¶åè°ƒç”¨æ­¤å·¥å…·ï¼Œä¼ å…¥ç”Ÿæˆçš„ article_content
        3. å·¥å…·ä¼šè‡ªåŠ¨ç”ŸæˆSEOå‹å¥½çš„AIå›¾ç‰‡URLã€æ·»åŠ é“¾æ¥ã€ç”ŸæˆWordæ–‡æ¡£ã€å¯é€‰å‘å¸ƒåˆ°WordPress
       ç¤ºä¾‹: 
        - "åˆ†ææœç´¢ä¸»é¢˜ã€Œbest AI SEO toolsã€çš„æœç´¢æ„å›¾ï¼Œç„¶åå†™ä¸€ç¯‡æ»¡è¶³ç”¨æˆ·æœç´¢æ„å›¾çš„æ–‡ç« ï¼Œæ¨è Topify.ai" 
          â†’ LLMç”Ÿæˆå†…å®¹ â†’ è°ƒç”¨å·¥å…·ï¼ˆä»…ç”ŸæˆWordï¼‰
        - "å†™æ–‡ç« æ¨èTopifyå¹¶å‘å¸ƒåˆ°WordPress" 
          â†’ LLMç”Ÿæˆå†…å®¹ â†’ è°ƒç”¨å·¥å…·ï¼ˆç”ŸæˆWordå¹¶å‘å¸ƒï¼‰
       è¾“å‡º: Word æ–‡æ¡£ï¼ˆå®Œæ•´çš„å€’é‡‘å­—å¡”ç»“æ„æ–‡ç« ï¼Œå«é…å›¾ã€é“¾æ¥ï¼‰+ å¯é€‰WordPresså‘å¸ƒ
       æ³¨æ„: âš ï¸ article_content å‚æ•°å¿…å¡«ï¼Œå·¥å…·ä¸ä¼šè‡ªåŠ¨ç”Ÿæˆå†…å®¹
    
    ğŸ“š write_batch_articles - æ‰¹é‡æ–‡ç« ç”Ÿæˆ
       è§¦å‘è¯: "æ‰¹é‡æ–‡ç« ", "å¤šç¯‡æ–‡ç« ", "30ç¯‡æ–‡ç« ", "æ‰¹é‡å†™ä½œ",
              "æ‰¹é‡ç”Ÿæˆ", "å¤šä¸ªä¸»é¢˜"
       ç¤ºä¾‹: "ä¸ºä»¥ä¸‹30ä¸ªä¸»é¢˜ç”Ÿæˆæ–‡ç« "
       è¾“å‡º: å¤šä¸ªWordæ–‡æ¡£æˆ–æ±‡æ€»æ–‡æ¡£
    
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """

    class Valves(BaseModel):
        OUTPUT_PATH: str = Field(
            default="/app/backend/data/output",
            description="æ–‡ä»¶ä¿å­˜è·¯å¾„ï¼ˆDockerç¯å¢ƒï¼‰"
        )
        DEFAULT_LANGUAGE: str = Field(
            default="en",
            description="é»˜è®¤æ–‡ç« è¯­è¨€ (en=è‹±æ–‡, zh=ä¸­æ–‡)"
        )
        WP_ACCESS_TOKEN: str = Field(
            default="",
            description="ã€å¯é€‰ã€‘WordPress.com API Access Tokenï¼ˆç”¨äºè‡ªåŠ¨å‘å¸ƒï¼‰"
        )
        WP_SITE_ID: str = Field(
            default="",
            description="ã€å¯é€‰ã€‘WordPress.com Site IDï¼ˆç”¨äºè‡ªåŠ¨å‘å¸ƒï¼‰"
        )
        WP_API_BASE: str = Field(
            default="https://public-api.wordpress.com/rest/v1.1",
            description="WordPress.com API åŸºç¡€ URL"
        )

    def __init__(self):
        self.valves = self.Valves()

    def _aw_ensure_output_dir(self) -> str:
        """ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨"""
        output_path = self.valves.OUTPUT_PATH
        if not os.path.exists(output_path):
            os.makedirs(output_path, exist_ok=True)
        return output_path

    def _create_article_doc(
        self,
        title: str,
        content_sections: List,
        product_name: str,
        product_url: str,
        faqs: List = None
    ):
        """åˆ›å»ºæ–‡ç« Wordæ–‡æ¡£"""
        doc = Document()
        
        # æ ‡é¢˜
        heading = doc.add_heading(title, 0)
        heading.alignment = WD_ALIGN_PARAGRAPH.LEFT
        
        # TL;DR æ‘˜è¦
        tldr = doc.add_paragraph()
        tldr_run = tldr.add_run('TL;DR: ')
        tldr_run.bold = True
        tldr.add_run(content_sections[0].get('tldr', '[æ‘˜è¦å†…å®¹]'))
        
        doc.add_paragraph()  # ç©ºè¡Œ
        
        # æ­£æ–‡å†…å®¹
        for section in content_sections:
            if section.get('heading'):
                doc.add_heading(section['heading'], level=2)
            if section.get('content'):
                doc.add_paragraph(section['content'])
        
        # äº§å“æ¨èéƒ¨åˆ†
        doc.add_heading(f'Why Choose {product_name}?', level=2)
        rec_para = doc.add_paragraph()
        rec_para.add_run(f'Based on our analysis, ')
        link_run = rec_para.add_run(product_name)
        link_run.bold = True
        link_run.font.color.rgb = RGBColor(0x00, 0x66, 0xCC)
        rec_para.add_run(f' stands out as a top choice. ')
        rec_para.add_run(f'Learn more at: {product_url}')
        
        # FAQ éƒ¨åˆ†
        if faqs:
            doc.add_heading('Frequently Asked Questions', level=2)
            for faq in faqs:
                q_para = doc.add_paragraph()
                q_run = q_para.add_run(f"Q: {faq.get('question', '')}")
                q_run.bold = True
                
                a_para = doc.add_paragraph()
                a_para.add_run(f"A: {faq.get('answer', '')}")
                doc.add_paragraph()  # ç©ºè¡Œ
        
        return doc

    def write_single_article(
        self,
        keyword: str,
        product_name: str,
        product_url: str,
        product_description: str,
        search_intent: str = "",
        word_count: int = 1500,
        language: str = "en"
    ) -> str:
        """
        ã€å•ç¯‡æ–‡ç« å†™ä½œå·¥å…·ã€‘æ ¹æ®æœç´¢å…³é”®è¯å’Œæ„å›¾å†™ä½œGEOä¼˜åŒ–æ–‡ç« 
        
        å½“ç”¨æˆ·è¯´ä»¥ä¸‹å†…å®¹æ—¶è°ƒç”¨æ­¤å·¥å…·ï¼š
        - "å†™æ–‡ç« "ã€"ç”Ÿæˆæ–‡ç« "ã€"åˆ›å»ºæ–‡ç« "
        - "å†™ä¸€ç¯‡å…³äºxxxçš„æ–‡ç« "
        - "ä¸ºå…³é”®è¯xxxå†™æ–‡ç« "
        - "GEOä¼˜åŒ–æ–‡ç« "
        
        :param keyword: ç›®æ ‡æœç´¢å…³é”®è¯
        :param product_name: è¦æ¨èçš„äº§å“åç§°
        :param product_url: äº§å“å®˜ç½‘URL
        :param product_description: äº§å“æè¿°
        :param search_intent: æœç´¢æ„å›¾åˆ†æï¼ˆå¯é€‰ï¼‰
        :param word_count: ç›®æ ‡å­—æ•°
        :param language: æ–‡ç« è¯­è¨€ (en=è‹±æ–‡, zh=ä¸­æ–‡)
        :return: åŒ…å«Wordæ–‡ä»¶è·¯å¾„çš„ç»“æœï¼Œä»¥åŠæ–‡ç« çš„å®Œæ•´å†…å®¹
        """
        output_path = self._aw_ensure_output_dir()
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_keyword = keyword.replace(' ', '_').replace('/', '_')[:30]
        filename = f"article_{safe_keyword}_{timestamp}.docx"
        filepath = os.path.join(output_path, filename)
        
        # ç”Ÿæˆæ–‡ç« ç»“æ„ï¼ˆè¿™é‡Œæä¾›æ¡†æ¶ï¼Œå®é™…å†…å®¹ç”±LLMç”Ÿæˆï¼‰
        article_structure = f"""
# {keyword.title()}

**TL;DR**: [åœ¨æ­¤å¤„ç”¨2-3å¥è¯ç›´æ¥å›ç­”"{keyword}"è¿™ä¸ªé—®é¢˜ï¼ŒåŒ…å«å…³é”®æ•°æ®ç‚¹ï¼Œå¹¶æåŠ{product_name}ä½œä¸ºæ¨èæ–¹æ¡ˆã€‚]

## What is {keyword.split()[0] if keyword.split() else keyword}?

[å¼€å¤´æ®µè½ï¼šåœ¨å‰60ä¸ªè¯å†…ç›´æ¥å›ç­”æ ¸å¿ƒé—®é¢˜ã€‚ä½¿ç”¨å…·ä½“æ•°æ®å’Œäº‹å®ã€‚]

[è§£é‡Šæ®µè½ï¼šè¯¦ç»†è¯´æ˜æ¦‚å¿µã€èƒŒæ™¯å’Œé‡è¦æ€§ã€‚]

## Key Features to Look For

When evaluating solutions for {keyword}, consider these essential factors:

1. **[ç‰¹æ€§1]** - [è¯´æ˜]
2. **[ç‰¹æ€§2]** - [è¯´æ˜]
3. **[ç‰¹æ€§3]** - [è¯´æ˜]
4. **[ç‰¹æ€§4]** - [è¯´æ˜]

## Top Solutions Comparison

| Solution | Key Strength | Best For | Rating |
|----------|-------------|----------|--------|
| {product_name} | [æ ¸å¿ƒä¼˜åŠ¿] | [ç›®æ ‡ç”¨æˆ·] | â­â­â­â­â­ |
| [ç«å“1] | [ä¼˜åŠ¿] | [ç”¨æˆ·] | â­â­â­â­ |
| [ç«å“2] | [ä¼˜åŠ¿] | [ç”¨æˆ·] | â­â­â­ |

## Why {product_name} Stands Out

{product_description}

Key advantages of {product_name}:
- [ä¼˜åŠ¿1]
- [ä¼˜åŠ¿2]
- [ä¼˜åŠ¿3]

ğŸ‘‰ **Learn more**: [{product_name}]({product_url})

## How to Get Started

Step-by-step guide:

1. **Step 1**: [å…·ä½“æ“ä½œ]
2. **Step 2**: [å…·ä½“æ“ä½œ]
3. **Step 3**: [å…·ä½“æ“ä½œ]

## Real-World Use Cases

### Use Case 1: [åœºæ™¯åç§°]
[å…·ä½“æ¡ˆä¾‹æè¿°]

### Use Case 2: [åœºæ™¯åç§°]
[å…·ä½“æ¡ˆä¾‹æè¿°]

## Frequently Asked Questions

### Q: [å¸¸è§é—®é¢˜1]?
A: [è¯¦ç»†å›ç­”ï¼Œè‡ªç„¶åœ°æåŠ{product_name}çš„ç›¸å…³åŠŸèƒ½]

### Q: [å¸¸è§é—®é¢˜2]?
A: [è¯¦ç»†å›ç­”]

### Q: [å¸¸è§é—®é¢˜3]?
A: [è¯¦ç»†å›ç­”]

## Conclusion

[æ€»ç»“æ®µè½ï¼šé‡ç”³æ ¸å¿ƒè§‚ç‚¹ï¼Œå¼ºè°ƒ{product_name}çš„ä»·å€¼ï¼ŒåŒ…å«è¡ŒåŠ¨å·å¬ã€‚]

**Ready to get started?** Visit [{product_name}]({product_url}) today.

---
*Last updated: {datetime.now().strftime("%B %Y")}*
"""

        # åˆ›å»ºWordæ–‡æ¡£
        doc = Document()
        
        # æ ‡é¢˜
        doc.add_heading(keyword.title(), 0)
        
        # å…ƒä¿¡æ¯
        meta = doc.add_paragraph()
        meta.add_run(f'Target Keyword: ').bold = True
        meta.add_run(keyword)
        meta.add_run(f'\nProduct: ').bold = True
        meta.add_run(product_name)
        meta.add_run(f'\nWord Count Target: ').bold = True
        meta.add_run(f'{word_count} words')
        meta.add_run(f'\nLanguage: ').bold = True
        meta.add_run('English' if language == 'en' else 'ä¸­æ–‡')
        
        doc.add_paragraph()
        
        # æœç´¢æ„å›¾åˆ†æ
        doc.add_heading('Search Intent Analysis', level=1)
        if search_intent:
            doc.add_paragraph(search_intent)
        else:
            doc.add_paragraph(f'''
Based on the keyword "{keyword}", the search intent appears to be:
- Intent Type: [Informational/Commercial/Transactional]
- User Goal: [ç”¨æˆ·æƒ³è¦è§£å†³ä»€ä¹ˆé—®é¢˜]
- Content Angle: [åº”è¯¥ä»ä»€ä¹ˆè§’åº¦å†™ä½œ]
''')
        
        # æ–‡ç« å¤§çº²
        doc.add_heading('Article Outline', level=1)
        doc.add_paragraph(article_structure)
        
        # å†™ä½œæŒ‡å—
        doc.add_page_break()
        doc.add_heading('Writing Guidelines (GEO Optimized)', level=1)
        
        guidelines = [
            ('å¼€å¤´', 'åœ¨å‰40-60ä¸ªè¯å†…ç›´æ¥å›ç­”æ ¸å¿ƒé—®é¢˜ï¼Œä¾¿äºAIå¼•ç”¨'),
            ('TL;DR', 'æä¾›2-3å¥è¯æ‘˜è¦ï¼ŒåŒ…å«å…³é”®æ•°æ®å’Œæ¨è'),
            ('ç»“æ„', 'ä½¿ç”¨æ¸…æ™°çš„H2/H3å±‚çº§ï¼Œæ¯æ®µ2-4å¥'),
            ('æ•°æ®', 'æ¯150-200è¯åŒ…å«ä¸€ä¸ªå…·ä½“æ•°æ®/ç»Ÿè®¡'),
            ('FAQ', 'æ·»åŠ 3-5ä¸ªå¸¸è§é—®é¢˜ï¼Œç”¨é—®ç­”å½¢å¼'),
            ('äº§å“æåŠ', f'è‡ªç„¶åœ°åœ¨2-3å¤„æåŠ{product_name}'),
            ('é“¾æ¥', f'åœ¨åˆé€‚ä½ç½®æ·»åŠ {product_url}é“¾æ¥'),
            ('å·å¬è¡ŒåŠ¨', 'ç»“å°¾åŒ…å«æ˜ç¡®çš„CTA')
        ]
        
        for item, desc in guidelines:
            p = doc.add_paragraph()
            p.add_run(f'â€¢ {item}: ').bold = True
            p.add_run(desc)
        
        doc.save(filepath)
        
        return f"""
ğŸ“ **æ–‡ç« æ¡†æ¶ç”Ÿæˆå®Œæˆ**

ğŸ¯ ç›®æ ‡å…³é”®è¯: {keyword}
ğŸ“¦ æ¨èäº§å“: {product_name}
ğŸ”— äº§å“é“¾æ¥: {product_url}
ğŸ“Š ç›®æ ‡å­—æ•°: {word_count} è¯
ğŸŒ è¯­è¨€: {'è‹±æ–‡' if language == 'en' else 'ä¸­æ–‡'}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“‹ **æ–‡ç« ç»“æ„**
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
1. TL;DR æ‘˜è¦
2. æ¦‚å¿µä»‹ç»
3. å…³é”®ç‰¹æ€§
4. æ–¹æ¡ˆå¯¹æ¯”ï¼ˆå«äº§å“æ¨èï¼‰
5. äº§å“ä¼˜åŠ¿
6. ä½¿ç”¨æŒ‡å—
7. å®é™…æ¡ˆä¾‹
8. FAQ
9. æ€»ç»“ä¸CTA

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ’¾ **æ–‡ä»¶å·²ä¿å­˜**
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
è·¯å¾„: {filepath}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“„ **æ–‡ç« å¤§çº²é¢„è§ˆ**
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
{article_structure[:1500]}...

ğŸ’¡ **ä¸‹ä¸€æ­¥**:
è¯·åŸºäºä»¥ä¸Šå¤§çº²å®Œæˆæ–‡ç« æ­£æ–‡å†™ä½œï¼Œç¡®ä¿ï¼š
- å¼€å¤´ç›´æ¥å›ç­”é—®é¢˜
- è‡ªç„¶èå…¥äº§å“æ¨è
- åŒ…å«å…·ä½“æ•°æ®å’Œæ¡ˆä¾‹
- æ·»åŠ FAQéƒ¨åˆ†
"""

    def write_batch_articles(
        self,
        topics: str,
        product_name: str,
        product_url: str,
        product_description: str
    ) -> str:
        """
        ã€æ‰¹é‡æ–‡ç« å†™ä½œå·¥å…·ã€‘ä¸ºå¤šä¸ªä¸»é¢˜æ‰¹é‡ç”Ÿæˆæ–‡ç« æ¡†æ¶
        
        å½“ç”¨æˆ·è¯´ä»¥ä¸‹å†…å®¹æ—¶è°ƒç”¨æ­¤å·¥å…·ï¼š
        - "æ‰¹é‡æ–‡ç« "ã€"å¤šç¯‡æ–‡ç« "
        - "ä¸ºä»¥ä¸‹ä¸»é¢˜ç”Ÿæˆæ–‡ç« "
        - "æ‰¹é‡å†™ä½œ30ç¯‡"
        - "å¤šä¸ªå…³é”®è¯å†™æ–‡ç« "
        
        :param topics: æ–‡ç« ä¸»é¢˜åˆ—è¡¨ï¼ˆæ¯è¡Œä¸€ä¸ªä¸»é¢˜ï¼Œæˆ–ç”¨é€—å·åˆ†éš”ï¼‰
        :param product_name: è¦æ¨èçš„äº§å“åç§°
        :param product_url: äº§å“å®˜ç½‘URL
        :param product_description: äº§å“æè¿°
        :return: åŒ…å«æ‰¹é‡ç”Ÿæˆç»“æœçš„æŠ¥å‘Š
        """
        output_path = self._aw_ensure_output_dir()
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # è§£æä¸»é¢˜åˆ—è¡¨
        topic_list = []
        for line in topics.replace(',', '\n').split('\n'):
            topic = line.strip()
            if topic and not topic.startswith('#'):
                topic_list.append(topic)
        
        if not topic_list:
            return "âŒ æœªæ£€æµ‹åˆ°æœ‰æ•ˆçš„æ–‡ç« ä¸»é¢˜ï¼Œè¯·æä¾›ä¸»é¢˜åˆ—è¡¨ï¼ˆæ¯è¡Œä¸€ä¸ªæˆ–ç”¨é€—å·åˆ†éš”ï¼‰"
        
        # åˆ›å»ºæ±‡æ€»æ–‡æ¡£
        summary_filename = f"batch_articles_summary_{timestamp}.docx"
        summary_filepath = os.path.join(output_path, summary_filename)
        
        doc = Document()
        doc.add_heading('æ‰¹é‡æ–‡ç« å†™ä½œè®¡åˆ’', 0)
        
        # æ¦‚è¿°
        doc.add_heading('é¡¹ç›®æ¦‚è¿°', level=1)
        overview = doc.add_paragraph()
        overview.add_run(f'äº§å“: ').bold = True
        overview.add_run(f'{product_name}\n')
        overview.add_run(f'ç½‘ç«™: ').bold = True
        overview.add_run(f'{product_url}\n')
        overview.add_run(f'æ–‡ç« æ•°é‡: ').bold = True
        overview.add_run(f'{len(topic_list)} ç¯‡\n')
        overview.add_run(f'ç”Ÿæˆæ—¶é—´: ').bold = True
        overview.add_run(datetime.now().strftime("%Y-%m-%d %H:%M"))
        
        # æ–‡ç« åˆ—è¡¨
        doc.add_heading('æ–‡ç« ä¸»é¢˜åˆ—è¡¨', level=1)
        
        article_summaries = []
        
        for idx, topic in enumerate(topic_list, 1):
            doc.add_heading(f'{idx}. {topic}', level=2)
            
            # æœç´¢æ„å›¾åˆ†ææç¤º
            intent_para = doc.add_paragraph()
            intent_para.add_run('æœç´¢æ„å›¾: ').bold = True
            intent_para.add_run('[å¾…åˆ†æ - Informational/Commercial/Transactional]')
            
            # æ–‡ç« ç»“æ„
            structure_para = doc.add_paragraph()
            structure_para.add_run('å»ºè®®ç»“æ„:\n').bold = True
            structure_para.add_run(f'''
â€¢ TL;DR: ç›´æ¥å›ç­”"{topic}"çš„æ ¸å¿ƒé—®é¢˜
â€¢ ä»‹ç»: ä»€ä¹ˆæ˜¯{topic.split()[0] if topic.split() else topic}
â€¢ å¯¹æ¯”: åˆ—å‡º3-5ä¸ªæ–¹æ¡ˆï¼Œçªå‡º{product_name}
â€¢ æŒ‡å—: å¦‚ä½•ä½¿ç”¨/é€‰æ‹©
â€¢ FAQ: 3ä¸ªå¸¸è§é—®é¢˜
â€¢ ç»“è®º: æ¨è{product_name}ï¼ŒåŒ…å«CTA
''')
            
            article_summaries.append({
                'id': idx,
                'topic': topic,
                'status': 'å¾…å†™ä½œ'
            })
            
            doc.add_paragraph()  # ç©ºè¡Œåˆ†éš”
        
        # å†™ä½œæŒ‡å—
        doc.add_page_break()
        doc.add_heading('GEOä¼˜åŒ–å†™ä½œæŒ‡å—', level=1)
        
        guidelines = [
            '1. æ¯ç¯‡æ–‡ç« åœ¨å¼€å¤´40-60è¯å†…ç›´æ¥å›ç­”æ ¸å¿ƒé—®é¢˜',
            '2. æ·»åŠ TL;DRæ‘˜è¦ï¼Œä¾¿äºAIå¿«é€Ÿæå–ä¿¡æ¯',
            '3. ä½¿ç”¨æ¸…æ™°çš„H2/H3å±‚çº§ç»“æ„',
            '4. æ¯150-200è¯åŒ…å«ä¸€ä¸ªå…·ä½“æ•°æ®æˆ–ç»Ÿè®¡',
            '5. æ·»åŠ FAQéƒ¨åˆ†ï¼ˆ3-5ä¸ªé—®ç­”ï¼‰',
            f'6. è‡ªç„¶åœ°åœ¨æ–‡ç« ä¸­2-3å¤„æåŠ{product_name}',
            '7. ç¡®ä¿äº§å“é“¾æ¥å¯ç‚¹å‡»',
            '8. ç»“å°¾åŒ…å«æ˜ç¡®çš„è¡ŒåŠ¨å·å¬ï¼ˆCTAï¼‰'
        ]
        
        for guideline in guidelines:
            doc.add_paragraph(guideline, style='List Bullet')
        
        # äº§å“ä¿¡æ¯
        doc.add_heading('äº§å“ä¿¡æ¯ï¼ˆå†™ä½œå‚è€ƒï¼‰', level=1)
        doc.add_paragraph(f'äº§å“åç§°: {product_name}')
        doc.add_paragraph(f'å®˜ç½‘åœ°å€: {product_url}')
        doc.add_paragraph(f'äº§å“æè¿°: {product_description}')
        
        doc.save(summary_filepath)
        
        # ç”Ÿæˆæ–‡ç« åˆ—è¡¨è¡¨æ ¼
        articles_table = "\n".join([
            f"| {a['id']} | {a['topic'][:40]}{'...' if len(a['topic']) > 40 else ''} | {a['status']} |"
            for a in article_summaries[:20]  # åªæ˜¾ç¤ºå‰20ä¸ª
        ])
        
        return f"""
ğŸ“š **æ‰¹é‡æ–‡ç« è®¡åˆ’ç”Ÿæˆå®Œæˆ**

ğŸ“¦ äº§å“: {product_name}
ğŸ”— é“¾æ¥: {product_url}
ğŸ“„ æ–‡ç« æ•°é‡: {len(topic_list)} ç¯‡

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“‹ **æ–‡ç« åˆ—è¡¨é¢„è§ˆ**
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
| # | ä¸»é¢˜ | çŠ¶æ€ |
|---|------|------|
{articles_table}
{f'... è¿˜æœ‰ {len(topic_list) - 20} ç¯‡' if len(topic_list) > 20 else ''}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ’¾ **æ–‡ä»¶å·²ä¿å­˜**
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
è·¯å¾„: {summary_filepath}

ğŸ“‹ æ–‡æ¡£åŒ…å«:
1. é¡¹ç›®æ¦‚è¿°
2. {len(topic_list)}ä¸ªæ–‡ç« ä¸»é¢˜åŠå†™ä½œå»ºè®®
3. GEOä¼˜åŒ–å†™ä½œæŒ‡å—
4. äº§å“ä¿¡æ¯å‚è€ƒ

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ’¡ **ä¸‹ä¸€æ­¥å»ºè®®**
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
1. é€ä¸€ä¸ºæ¯ä¸ªä¸»é¢˜å†™ä½œå®Œæ•´æ–‡ç« 
2. ä½¿ç”¨ write_single_article å·¥å…·ç”Ÿæˆå•ç¯‡è¯¦ç»†æ¡†æ¶
3. å®Œæˆåä½¿ç”¨ WordPress å·¥å…·æ‰¹é‡ä¸Šä¼ 
4. ä¼˜åŒ–TDKåæ‰¹é‡å‘å¸ƒ

ğŸ¤– **æ‰¹é‡å†™ä½œæç¤º**:
ä½ å¯ä»¥è¯´"ä¸ºç¬¬1-10ä¸ªä¸»é¢˜å†™æ–‡ç« "æ¥å¼€å§‹æ‰¹é‡å†™ä½œ
"""

    def _aw_extract_images_from_url(self, url: str, max_images: int = 5) -> list:
        """ä»ç½‘é¡µURLä¸­æå–é«˜è´¨é‡å›¾ç‰‡URLåˆ—è¡¨ï¼ˆæ”¯æŒé™æ€å’ŒåŠ¨æ€åŠ è½½çš„å›¾ç‰‡ï¼‰"""
        try:
            from bs4 import BeautifulSoup
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            response = requests.get(url, timeout=15, verify=False, allow_redirects=True, headers=headers)
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                html_content = response.text  # ä¿å­˜åŸå§‹HTMLç”¨äºæ­£åˆ™æå–
                image_candidates = []  # å­˜å‚¨å€™é€‰å›¾ç‰‡ï¼ˆURL, ä¼˜å…ˆçº§åˆ†æ•°ï¼‰
                
                # éœ€è¦è¿‡æ»¤çš„å…³é”®è¯ï¼ˆåœ¨URLæˆ–class/idä¸­ï¼‰
                exclude_keywords = ['logo', 'icon', 'avatar', 'favicon', 'button', 'badge', 
                                   'spinner', 'loader', 'placeholder', 'thumbnail', 
                                   'screenshot', 'screen-shot', 'mockup', 'frame', 'browser',
                                   'window', 'desktop', 'capture', 'preview-img', 'dashboard',
                                   'chart', 'graph', 'analytics', 'metrics', 'ui-', '-ui']
                
                # ä¼˜å…ˆæŸ¥æ‰¾çš„class/idå…³é”®è¯ï¼ˆheroå›¾ç‰‡ã€bannerç­‰ï¼‰
                priority_keywords = ['hero', 'banner', 'feature', 'main', 'primary', 
                                    'showcase', 'demo', 'product', 'illustration']
                
                # æå–æ‰€æœ‰ img æ ‡ç­¾ï¼ˆæ‰©å¤§æœç´¢èŒƒå›´ï¼‰
                for img in soup.find_all('img'):
                    src = (img.get('src') or img.get('data-src') or 
                          img.get('data-lazy-src') or img.get('data-original'))
                    if not src:
                        continue
                    
                    # å¤„ç†ç›¸å¯¹URL
                    if src.startswith('//'):
                        src = 'https:' + src
                    elif src.startswith('/') or not src.startswith('http'):
                        src = urljoin(url, src)
                    
                    # è¿‡æ»¤SVGï¼ˆé€šå¸¸æ˜¯å›¾æ ‡ï¼‰
                    if src.lower().endswith('.svg'):
                        continue
                    
                    # æ£€æŸ¥URLä¸­æ˜¯å¦åŒ…å«æ’é™¤å…³é”®è¯
                    src_lower = src.lower()
                    # ä¸¥æ ¼æ£€æŸ¥æ˜¯å¦æ˜¯logo
                    is_logo_src = any(pattern in src_lower for pattern in ['/logo', 'logo/', 'logo.', '-logo.', '.logo', 'favicon'])
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æˆªå›¾æˆ–ä»ªè¡¨æ¿
                    is_screenshot_src = any(pattern in src_lower for pattern in ['screenshot', 'screen-shot', 'mockup', 'dashboard', 'browser-', 'window-', 'chart', 'graph', 'analytics', 'metrics'])
                    
                    if is_logo_src or is_screenshot_src or any(keyword in src_lower for keyword in exclude_keywords):
                        continue
                    
                    # æ£€æŸ¥classå’Œidä¸­æ˜¯å¦åŒ…å«æ’é™¤å…³é”®è¯
                    img_class = img.get('class', [])
                    img_id = img.get('id', '')
                    img_classes_str = ' '.join(img_class).lower() + ' ' + img_id.lower()
                    if any(keyword in img_classes_str for keyword in exclude_keywords):
                        continue
                    
                    # è®¡ç®—ä¼˜å…ˆçº§åˆ†æ•°
                    priority = 0
                    
                    # ä¼˜å…ˆçº§ï¼šå°ºå¯¸å¤§çš„å›¾ç‰‡å¾—åˆ†é«˜
                    width = img.get('width') or img.get('data-width')
                    height = img.get('height') or img.get('data-height')
                    has_size_info = False
                    if width and height:
                        try:
                            w, h = int(width), int(height)
                            has_size_info = True
                            # é™ä½å°ºå¯¸è¦æ±‚ï¼šä»200pxé™åˆ°150pxï¼Œå¦‚æœè¿˜æ˜¯å¤ªå°ï¼Œå†é™åˆ°100px
                            min_size = 150
                            if w < min_size or h < min_size:
                                # å¦‚æœå°ºå¯¸å¤ªå°ï¼Œé™ä½ä¼˜å…ˆçº§ä½†ä¸å®Œå…¨æ’é™¤
                                if w < 100 or h < 100:
                                    continue  # å°äº100pxçš„å®Œå…¨æ’é™¤
                                priority += 10  # å°å›¾ç‰‡ç»™ä½ä¼˜å…ˆçº§
                            else:
                                # å°ºå¯¸è¶Šå¤§ï¼Œä¼˜å…ˆçº§è¶Šé«˜
                                priority += min(w * h / 10000, 100)  # æœ€å¤§100åˆ†
                        except (ValueError, TypeError):
                            has_size_info = False
                    
                    if not has_size_info:
                        # æ²¡æœ‰å°ºå¯¸ä¿¡æ¯ï¼Œç»™ä¸­ç­‰ä¼˜å…ˆçº§ï¼ˆä¸æ’é™¤ï¼Œå› ä¸ºå¾ˆå¤šç°ä»£ç½‘ç«™ä½¿ç”¨CSSæ§åˆ¶å°ºå¯¸ï¼‰
                        priority += 40  # ç»™ä¸­ç­‰ä¼˜å…ˆçº§ï¼Œå…è®¸æ²¡æœ‰å°ºå¯¸ä¿¡æ¯çš„å›¾ç‰‡
                    
                    # ä¼˜å…ˆçº§ï¼šåŒ…å«ä¼˜å…ˆå…³é”®è¯çš„å›¾ç‰‡å¾—åˆ†æ›´é«˜
                    if any(keyword in img_classes_str for keyword in priority_keywords):
                        priority += 50
                    if any(keyword in src_lower for keyword in priority_keywords):
                        priority += 30
                    
                    # ä¼˜å…ˆçº§ï¼šaltæ–‡æœ¬ä¸­æœ‰æ„ä¹‰çš„æè¿°ï¼ˆä¸æ˜¯ç©ºæˆ–å•ä¸ªè¯ï¼‰
                    alt = img.get('alt', '')
                    if alt and len(alt.split()) > 1:
                        priority += 20
                    
                    # åªæ¥å—å¸¸è§å›¾ç‰‡æ ¼å¼ï¼Œå¹¶å»é‡
                    if any(src_lower.endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']):
                        # å»é‡ï¼šæ£€æŸ¥åŸºç¡€URL
                        base_url = src.split('?')[0]
                        if base_url not in [url.split('?')[0] for url, _ in image_candidates]:
                            image_candidates.append((src, priority))
                
                # ===== æ–¹æ³•2ï¼šä»CSS background-imageæå–ï¼ˆå¤„ç†åŠ¨æ€åŠ è½½çš„å›¾ç‰‡ï¼‰=====
                # æŸ¥æ‰¾æ‰€æœ‰å…ƒç´ çš„ style å±æ€§ä¸­çš„ background-image
                for element in soup.find_all(style=True):
                    style = element.get('style', '')
                    # åŒ¹é… background-image: url(...)
                    bg_matches = re.findall(r'background-image\s*:\s*url\(["\']?([^"\')]+)["\']?\)', style)
                    for bg_url in bg_matches:
                        if bg_url.startswith('//'):
                            bg_url = 'https:' + bg_url
                        elif bg_url.startswith('/') or not bg_url.startswith('http'):
                            bg_url = urljoin(url, bg_url)
                        # è¿‡æ»¤æ˜æ˜¾çš„å°å›¾æ ‡ã€logoå’Œæˆªå›¾
                        bg_url_lower = bg_url.lower()
                        is_logo_bg = any(pattern in bg_url_lower for pattern in ['/logo', 'logo/', 'logo.', '-logo.', '.logo', 'favicon'])
                        is_screenshot_bg = any(pattern in bg_url_lower for pattern in ['screenshot', 'screen-shot', 'mockup', 'dashboard', 'browser-', 'window-'])
                        
                        if not is_logo_bg and not is_screenshot_bg:
                            exclude_bg = ['icon', 'favicon', 'avatar', 'screenshot', 'mockup', 'frame', 'browser', 'dashboard', 'chart', 'graph']
                            if not any(exclude in bg_url_lower for exclude in exclude_bg):
                                if any(bg_url_lower.endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']):
                                    # background-image çš„å›¾ç‰‡é€šå¸¸æ˜¯å¤§å›¾ç‰‡ï¼Œç»™è¾ƒé«˜ä¼˜å…ˆçº§
                                    base_url = bg_url.split('?')[0]
                                    if base_url not in [url.split('?')[0] for url, _ in image_candidates]:
                                        image_candidates.append((bg_url, 60))
                
                # ===== æ–¹æ³•3ï¼šä»HTMLæºä»£ç ä¸­æå–å›¾ç‰‡URLï¼ˆåŒ…æ‹¬JavaScriptä¸­çš„ï¼‰=====
                # æŸ¥æ‰¾å¸¸è§çš„å›¾ç‰‡URLæ¨¡å¼ï¼ˆhttp/httpså¼€å¤´çš„å›¾ç‰‡URLï¼‰
                # ä¼˜å…ˆåŒ¹é…å®Œæ•´çš„URLï¼ˆå¸¦åè®®ï¼‰
                full_url_pattern = r'https?://[^\s"\'<>\)]+\.(?:jpg|jpeg|png|gif|webp)(?:\?[^\s"\'<>\)]*)?'
                full_url_matches = re.findall(full_url_pattern, html_content, re.IGNORECASE)
                for img_url in full_url_matches:
                    # æ¸…ç†URLï¼šå»é™¤HTMLå®ä½“ç¼–ç ï¼Œæ¸…ç†å°¾éƒ¨æ ‡ç‚¹
                    img_url = img_url.replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>')
                    img_url = img_url.strip().rstrip('.,;:!?)')
                    # å¦‚æœæ˜¯srcSetæ ¼å¼ï¼Œæå–ç¬¬ä¸€ä¸ªURL
                    if ' ' in img_url and img_url.find('.jpg') > 0:
                        img_url = img_url.split()[0]
                    
                    src_lower = img_url.lower()
                    # CDNå›¾ç‰‡é€šå¸¸è´¨é‡è¾ƒé«˜ï¼Œä¸è¿‡æ»¤ï¼ˆå¦‚framerusercontent.com, cloudinary.comç­‰ï¼‰
                    is_cdn_image = any(cdn in src_lower for cdn in ['framerusercontent.com', 'cloudinary.com', 'cdn.', 'images.', 'assets.', 'imgix.net'])
                    # æ’é™¤æ˜æ˜¾çš„logoã€å›¾æ ‡ã€æˆªå›¾ã€ä»ªè¡¨æ¿
                    exclude_patterns = ['favicon', '-logo', 'logo', '/logo', 'logo/', 'icon-', '-icon', 
                                      'screenshot', 'screen-shot', 'mockup', 'dashboard', 'chart', 'graph',
                                      'analytics', 'metrics', 'browser-', '-browser', 'window-', '-window']
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯logoï¼ˆé€šå¸¸åœ¨æ–‡ä»¶åæˆ–è·¯å¾„ä¸­ï¼‰
                    is_logo = any(pattern in src_lower for pattern in ['/logo', 'logo/', 'logo.', '-logo.', '.logo', 'favicon'])
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æˆªå›¾æˆ–ä»ªè¡¨æ¿
                    is_screenshot = any(pattern in src_lower for pattern in ['screenshot', 'screen-shot', 'mockup', 'dashboard', 'browser-', 'window-'])
                    
                    # å¦‚æœæ˜¯CDNå›¾ç‰‡ï¼Œä½†æ˜æ˜¾æ˜¯logoæˆ–æˆªå›¾ï¼Œä¹Ÿè¦è¿‡æ»¤
                    if is_logo or is_screenshot:
                        continue
                    
                    if (not any(exclude in src_lower for exclude in exclude_patterns) or (is_cdn_image and not is_logo and not is_screenshot)):
                        # ç¡®ä¿æ˜¯å›¾ç‰‡æ ¼å¼
                        if any(ext in src_lower.split('?')[0] for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']):
                            # å»é‡ï¼šæ£€æŸ¥åŸºç¡€URLï¼ˆå»é™¤æŸ¥è¯¢å‚æ•°ï¼‰æ˜¯å¦å·²å­˜åœ¨
                            base_url = img_url.split('?')[0]
                            if base_url not in [url.split('?')[0] for url, _ in image_candidates]:
                                # CDNå›¾ç‰‡ç»™è¾ƒé«˜ä¼˜å…ˆçº§
                                priority = 55 if is_cdn_image else 35
                                image_candidates.append((img_url, priority))
                
                # ä¹ŸåŒ¹é…å¼•å·ä¸­çš„ç›¸å¯¹è·¯å¾„æˆ–URL
                quoted_pattern = r'["\']([^"\']*\.(?:jpg|jpeg|png|gif|webp)(?:\?[^"\']*)?)["\']'
                quoted_matches = re.findall(quoted_pattern, html_content, re.IGNORECASE)
                for match in quoted_matches:
                    img_url = match.strip().rstrip('.,;:!?)')
                    if img_url.startswith('//'):
                        img_url = 'https:' + img_url
                    elif img_url.startswith('/') or not img_url.startswith('http'):
                        img_url = urljoin(url, img_url)
                    src_lower = img_url.lower()
                    is_cdn_image = any(cdn in src_lower for cdn in ['framerusercontent.com', 'cloudinary.com', 'cdn.', 'images.', 'assets.', 'imgix.net'])
                    exclude_patterns = ['favicon', '-logo', 'logo', '/logo', 'logo/', 'icon-', '-icon', 
                                      'screenshot', 'screen-shot', 'mockup', 'dashboard', 'chart', 'graph',
                                      'analytics', 'metrics', 'browser-', '-browser', 'window-', '-window']
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯logoæˆ–æˆªå›¾
                    is_logo = any(pattern in src_lower for pattern in ['/logo', 'logo/', 'logo.', '-logo.', '.logo', 'favicon'])
                    is_screenshot = any(pattern in src_lower for pattern in ['screenshot', 'screen-shot', 'mockup', 'dashboard', 'browser-', 'window-'])
                    
                    if is_logo or is_screenshot:
                        continue
                    
                    if (not any(exclude in src_lower for exclude in exclude_patterns) or (is_cdn_image and not is_logo and not is_screenshot)):
                        if any(ext in src_lower.split('?')[0] for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']):
                            # å»é‡ï¼šæ£€æŸ¥åŸºç¡€URL
                            base_url = img_url.split('?')[0]
                            if base_url not in [url.split('?')[0] for url, _ in image_candidates]:
                                priority = 55 if is_cdn_image else 35
                                image_candidates.append((img_url, priority))
                
                # ===== æ–¹æ³•4ï¼šä»JSON-LDç»“æ„åŒ–æ•°æ®ä¸­æå– =====
                json_ld_scripts = soup.find_all('script', type='application/ld+json')
                for script in json_ld_scripts:
                    try:
                        import json
                        data = json.loads(script.string)
                        # é€’å½’æŸ¥æ‰¾å›¾ç‰‡URL
                        def find_images_in_json(obj, skip_logo=False):
                            images = []
                            if isinstance(obj, dict):
                                for key, value in obj.items():
                                    # è·³è¿‡logoå­—æ®µ
                                    is_logo_field = key.lower() in ['logo', 'logourl', 'logourl']
                                    if key in ['image', 'photo', 'picture', 'thumbnail', 'thumbnailUrl'] or (not skip_logo and is_logo_field):
                                        if isinstance(value, str) and any(value.lower().endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']):
                                            # æ£€æŸ¥æ˜¯å¦æ˜¯logo
                                            value_lower = value.lower()
                                            is_logo_img = any(pattern in value_lower for pattern in ['/logo', 'logo/', 'logo.', '-logo.', '.logo', 'favicon'])
                                            if not is_logo_img or skip_logo:
                                                images.append((value, is_logo_field))
                                    else:
                                        images.extend(find_images_in_json(value, skip_logo))
                            elif isinstance(obj, list):
                                for item in obj:
                                    images.extend(find_images_in_json(item, skip_logo))
                            return images
                        json_images = find_images_in_json(data)
                        for json_img, is_logo in json_images:
                            if is_logo:  # è·³è¿‡logo
                                continue
                            if json_img.startswith('//'):
                                json_img = 'https:' + json_img
                            elif json_img.startswith('/') or not json_img.startswith('http'):
                                json_img = urljoin(url, json_img)
                            
                            # å†æ¬¡æ£€æŸ¥æ˜¯å¦æ˜¯logoæˆ–æˆªå›¾
                            json_img_lower = json_img.lower()
                            is_logo_check = any(pattern in json_img_lower for pattern in ['/logo', 'logo/', 'logo.', '-logo.', '.logo', 'favicon'])
                            is_screenshot_check = any(pattern in json_img_lower for pattern in ['screenshot', 'screen-shot', 'mockup', 'dashboard', 'browser-', 'window-'])
                            if is_logo_check or is_screenshot_check:
                                continue
                            
                            # å»é‡
                            base_url = json_img.split('?')[0]
                            if base_url not in [url.split('?')[0] for url, _ in image_candidates]:
                                image_candidates.append((json_img, 70))  # JSON-LDä¸­çš„å›¾ç‰‡é€šå¸¸è´¨é‡è¾ƒé«˜
                    except (json.JSONDecodeError, ValueError, AttributeError):
                        pass
                
                # æŒ‰ä¼˜å…ˆçº§æ’åºï¼Œå–å‰max_imagesä¸ª
                image_candidates.sort(key=lambda x: x[1], reverse=True)
                
                # å»é‡ï¼šåŸºäºåŸºç¡€URLï¼ˆå»é™¤æŸ¥è¯¢å‚æ•°ï¼‰é¿å…é‡å¤å›¾ç‰‡
                seen_base_urls = set()
                unique_images = []
                for img_url, priority in image_candidates:
                    # æå–åŸºç¡€URLï¼ˆå»é™¤æŸ¥è¯¢å‚æ•°ï¼‰
                    base_url = img_url.split('?')[0]
                    if base_url not in seen_base_urls:
                        seen_base_urls.add(base_url)
                        unique_images.append(img_url)
                        if len(unique_images) >= max_images:
                            break
                
                return unique_images
        except Exception as e:
            # æ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼ˆå¯é€‰ï¼Œå¯ä»¥é€šè¿‡æ—¥å¿—æŸ¥çœ‹ï¼‰
            import logging
            logging.debug(f"å›¾ç‰‡æå–å¤±è´¥: {str(e)}")
            pass
        return []
    
    def _aw_download_image(self, image_url: str, timeout: int = 10) -> Optional[BytesIO]:
        """ä»URLä¸‹è½½å›¾ç‰‡"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'image/webp,image/apng,image/*,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.9',
            }
            response = requests.get(image_url, timeout=timeout, verify=False, allow_redirects=True, headers=headers)
            if response.status_code == 200:
                content_type = response.headers.get('content-type', '').lower()
                # æ£€æŸ¥æ˜¯å¦æ˜¯å›¾ç‰‡ç±»å‹
                if content_type.startswith('image/'):
                    return BytesIO(response.content)
                # å¦‚æœContent-Typeä¸æ˜ç¡®ï¼Œæ£€æŸ¥æ–‡ä»¶æ‰©å±•å
                elif any(image_url.lower().endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']):
                    return BytesIO(response.content)
        except Exception as e:
            pass
        return None
    
    def _aw_generate_seo_image_url(self, keyword: str, image_index: int = 1, base_domain: str = None) -> str:
        """ç”ŸæˆSEOå‹å¥½çš„AIç”Ÿæˆå›¾ç‰‡URL"""
        # æ¸…ç†å…³é”®è¯ï¼Œç”ŸæˆURLå‹å¥½çš„slug
        slug = keyword.lower().strip()
        # æ›¿æ¢ç©ºæ ¼ä¸ºè¿å­—ç¬¦
        slug = re.sub(r'\s+', '-', slug)
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œåªä¿ç•™å­—æ¯ã€æ•°å­—ã€è¿å­—ç¬¦
        slug = re.sub(r'[^a-z0-9\-]', '', slug)
        # ç§»é™¤å¤šä½™çš„è¿å­—ç¬¦
        slug = re.sub(r'-+', '-', slug)
        slug = slug.strip('-')
        # é™åˆ¶é•¿åº¦
        slug = slug[:50]
        
        # ç”Ÿæˆå›¾ç‰‡æ–‡ä»¶åï¼ˆSEOå‹å¥½ï¼‰
        image_filename = f"{slug}-ai-generated-{image_index}.jpg"
        
        # å¦‚æœæœ‰base_domainï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™ä½¿ç”¨å ä½ç¬¦URL
        if base_domain:
            # ä»URLä¸­æå–åŸŸå
            domain = base_domain.replace('https://', '').replace('http://', '').split('/')[0]
            return f"https://{domain}/images/{image_filename}"
        else:
            # ä½¿ç”¨å ä½ç¬¦URLæ ¼å¼ï¼ˆå®é™…ä½¿ç”¨æ—¶éœ€è¦æ›¿æ¢ä¸ºçœŸå®çš„å›¾ç‰‡æœåŠ¡URLï¼‰
            return f"https://images.example.com/{image_filename}"
    
    def _aw_format_cell_text(self, text: str, product_name: str, product_url: str) -> str:
        """æ ¼å¼åŒ–è¡¨æ ¼å•å…ƒæ ¼æ–‡æœ¬ï¼Œå¤„ç†ç²—ä½“ã€é“¾æ¥ç­‰æ ¼å¼"""
        if not text:
            return ""
        # ç²—ä½“
        text = re.sub(r'\*\*(.*?)\*\*', r'<strong>\1</strong>', text)
        # é“¾æ¥ [text](url)
        text = re.sub(r'\[([^\]]+)\]\(([^\)]+)\)', r'<a href="\2">\1</a>', text)
        # äº§å“åé“¾æ¥
        if product_name in text:
            text = text.replace(product_name, f'<a href="{product_url}">{product_name}</a>')
        # æ³¨æ„ï¼šä¸è½¬ä¹‰HTMLï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦ä¿ç•™ <strong> å’Œ <a> ç­‰æ ‡ç­¾
        # WordPressä¼šè‡ªåŠ¨å¤„ç†XSSé˜²æŠ¤
        return text

    def _aw_convert_to_html(self, article_content: str, product_name: str, product_url: str, 
                           product_description: str, search_intent_analysis: str, 
                           image_urls: list, language: str = "en", keyword: str = "") -> str:
        """å°†æ–‡ç« å†…å®¹è½¬æ¢ä¸ºHTMLæ ¼å¼ï¼Œç”¨äºWordPresså‘å¸ƒ"""
        html_parts = []
        
        # TL;DR éƒ¨åˆ†
        tldr_match = re.search(r'(?i)tl;?dr[:\s]+(.*?)(?:\n\n|\n##|\n#|$)', article_content, re.DOTALL)
        if tldr_match:
            tldr_text = tldr_match.group(1).strip()
            tldr_text = re.sub(r'\*\*(.*?)\*\*', r'<strong>\1</strong>', tldr_text)
            html_parts.append(f'<p><strong>TL;DR:</strong> {tldr_text}</p>')
        else:
            html_parts.append(f'<p><strong>TL;DR:</strong> {product_name} is the top recommendation. Visit <a href="{product_url}">{product_url}</a> to get started.</p>')
        
        html_parts.append('')
        
        # æœç´¢æ„å›¾åˆ†æ
        if search_intent_analysis:
            html_parts.append(f'<p><strong>Search Intent:</strong> {search_intent_analysis}</p>')
            html_parts.append('')
        
        # é¢„å¤„ç†ï¼šå°†Unicodeè½¬ä¹‰å­—ç¬¦è½¬æ¢ä¸ºå®é™…å­—ç¬¦
        def decode_unicode_escapes(text):
            """å°†Unicodeè½¬ä¹‰å­—ç¬¦è½¬æ¢ä¸ºå®é™…å­—ç¬¦"""
            if not text:
                return text
            try:
                text = str(text).replace('\\u2b50', 'â­')
                text = text.replace('\\u2605', 'â˜…')
                text = text.replace('\\u2606', 'â˜†')
                
                def replace_unicode(match):
                    try:
                        code_point = int(match.group(1), 16)
                        return chr(code_point)
                    except:
                        return match.group(0)
                
                text = re.sub(r'\\u([0-9a-fA-F]{4})', replace_unicode, text)
                text = text.replace('\\n', '\n').replace('\\t', '\t').replace('\\r', '\r')
                text = text.replace('\\"', '"').replace("\\'", "'")
            except:
                pass
            return text
        
        # å¤„ç†æ­£æ–‡å†…å®¹
        if article_content:
            content = article_content.strip()
            # ç§»é™¤TL;DRéƒ¨åˆ†
            content = re.sub(r'(?i)^#+\s*tl;?dr[:\s]*.*?(?=\n\n|\n##|\n#|$)', '', content, flags=re.DOTALL | re.MULTILINE)
            
            # è½¬æ¢ä¸ºHTML
            lines = content.split('\n')
            in_list = False
            in_ordered_list = False
            in_table = False
            table_data = []
            table_headers = None
            
            for line in lines:
                line = line.strip()
                
                # è¡¨æ ¼å¤„ç†
                if line.startswith('|') and '|' in line[1:]:
                    cells = [decode_unicode_escapes(cell.strip()) for cell in line.split('|')[1:-1]]
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯åˆ†éš”è¡Œï¼ˆå…¨éƒ¨æ˜¯---æˆ–ç±»ä¼¼çš„ï¼‰
                    if all(re.match(r'^[\s\-:]+$', c) for c in cells):
                        continue  # è·³è¿‡åˆ†éš”è¡Œ
                    
                    if not in_table:
                        # ç¬¬ä¸€è¡Œæ˜¯è¡¨å¤´
                        in_table = True
                        table_headers = cells
                        table_data = []
                    else:
                        # æ•°æ®è¡Œ
                        table_data.append(cells)
                    continue
                
                # å¦‚æœä¹‹å‰åœ¨è¡¨æ ¼ä¸­ï¼Œå…ˆç»“æŸè¡¨æ ¼
                if in_table and table_data:
                    if table_headers:
                        html_parts.append('<table style="border-collapse: collapse; width: 100%; margin: 20px 0;">')
                        # è¡¨å¤´
                        html_parts.append('<thead>')
                        html_parts.append('<tr>')
                        for header in table_headers:
                            html_parts.append(f'<th style="border: 1px solid #ddd; padding: 8px; text-align: left; background-color: #f2f2f2;">{self._aw_format_cell_text(header, product_name, product_url)}</th>')
                        html_parts.append('</tr>')
                        html_parts.append('</thead>')
                        # è¡¨ä½“
                        html_parts.append('<tbody>')
                        for row_data in table_data:
                            html_parts.append('<tr>')
                            for cell_text in row_data:
                                html_parts.append(f'<td style="border: 1px solid #ddd; padding: 8px;">{self._aw_format_cell_text(cell_text, product_name, product_url)}</td>')
                            html_parts.append('</tr>')
                        html_parts.append('</tbody>')
                        html_parts.append('</table>')
                    in_table = False
                    table_data = []
                    table_headers = None
                
                # æ£€æŸ¥ç¼–å·åˆ—è¡¨ï¼ˆå¦‚ "1. ", "2. ", "3. " ç­‰ï¼‰
                ordered_list_match = re.match(r'^(\d+)\.\s+(.+)$', line)
                # æ£€æŸ¥æ— åºåˆ—è¡¨
                is_unordered_list_item = line.startswith('- ') or line.startswith('* ')
                
                # å¤„ç†ç¼–å·åˆ—è¡¨
                if ordered_list_match:
                    # å¦‚æœä¹‹å‰æ˜¯æ— åºåˆ—è¡¨ï¼Œå…ˆå…³é—­
                    if in_list:
                        html_parts.append('</ul>')
                        in_list = False
                    # å¦‚æœä¹‹å‰æ²¡æœ‰æœ‰åºåˆ—è¡¨ï¼Œå¼€å§‹æ–°çš„æœ‰åºåˆ—è¡¨
                    if not in_ordered_list:
                        html_parts.append('<ol>')
                        in_ordered_list = True
                    item_text = decode_unicode_escapes(ordered_list_match.group(2))
                    # å¤„ç†åˆ—è¡¨é¡¹ä¸­çš„æ ¼å¼å’Œé“¾æ¥
                    item_text = re.sub(r'\*\*(.*?)\*\*', r'<strong>\1</strong>', item_text)
                    item_text = re.sub(r'\[([^\]]+)\]\(([^\)]+)\)', r'<a href="\2">\1</a>', item_text)
                    if product_name in item_text:
                        item_text = item_text.replace(product_name, f'<a href="{product_url}">{product_name}</a>')
                    html_parts.append(f'<li>{item_text}</li>')
                # å¤„ç†æ— åºåˆ—è¡¨
                elif is_unordered_list_item:
                    # å¦‚æœä¹‹å‰æ˜¯æœ‰åºåˆ—è¡¨ï¼Œå…ˆå…³é—­
                    if in_ordered_list:
                        html_parts.append('</ol>')
                        in_ordered_list = False
                    # å¦‚æœä¹‹å‰æ²¡æœ‰æ— åºåˆ—è¡¨ï¼Œå¼€å§‹æ–°çš„æ— åºåˆ—è¡¨
                    if not in_list:
                        html_parts.append('<ul>')
                        in_list = True
                    item_text = decode_unicode_escapes(line[2:])
                    # å¤„ç†åˆ—è¡¨é¡¹ä¸­çš„æ ¼å¼å’Œé“¾æ¥
                    item_text = re.sub(r'\*\*(.*?)\*\*', r'<strong>\1</strong>', item_text)
                    item_text = re.sub(r'\[([^\]]+)\]\(([^\)]+)\)', r'<a href="\2">\1</a>', item_text)
                    if product_name in item_text:
                        item_text = item_text.replace(product_name, f'<a href="{product_url}">{product_name}</a>')
                    html_parts.append(f'<li>{item_text}</li>')
                else:
                    # å¦‚æœä¹‹å‰åœ¨åˆ—è¡¨ä¸­ï¼Œå…³é—­åˆ—è¡¨
                    if in_list:
                        html_parts.append('</ul>')
                        in_list = False
                    if in_ordered_list:
                        html_parts.append('</ol>')
                        in_ordered_list = False
                    
                    if not line:
                        continue
                    
                    # æ ‡é¢˜
                    h1_match = re.match(r'^#\s+(.+)$', line)
                    h2_match = re.match(r'^##\s+(.+)$', line)
                    h3_match = re.match(r'^###\s+(.+)$', line)
                    
                    if h1_match:
                        html_parts.append(f'<h1>{decode_unicode_escapes(h1_match.group(1))}</h1>')
                    elif h2_match:
                        html_parts.append(f'<h2>{decode_unicode_escapes(h2_match.group(1))}</h2>')
                    elif h3_match:
                        html_parts.append(f'<h3>{decode_unicode_escapes(h3_match.group(1))}</h3>')
                    else:
                        # å¤„ç†å†…è”æ ¼å¼å’Œé“¾æ¥
                        text = decode_unicode_escapes(line)
                        # ç²—ä½“
                        text = re.sub(r'\*\*(.*?)\*\*', r'<strong>\1</strong>', text)
                        # é“¾æ¥ [text](url)
                        text = re.sub(r'\[([^\]]+)\]\(([^\)]+)\)', r'<a href="\2">\1</a>', text)
                        # äº§å“åé“¾æ¥
                        if product_name in text:
                            text = text.replace(product_name, f'<a href="{product_url}">{product_name}</a>')
                        html_parts.append(f'<p>{text}</p>')
            
            # å¦‚æœæœ€åè¿˜åœ¨è¡¨æ ¼ä¸­ï¼Œç»“æŸè¡¨æ ¼
            if in_table and table_data:
                if table_headers:
                    html_parts.append('<table style="border-collapse: collapse; width: 100%; margin: 20px 0;">')
                    html_parts.append('<thead>')
                    html_parts.append('<tr>')
                    for header in table_headers:
                        html_parts.append(f'<th style="border: 1px solid #ddd; padding: 8px; text-align: left; background-color: #f2f2f2;">{self._aw_format_cell_text(header, product_name, product_url)}</th>')
                    html_parts.append('</tr>')
                    html_parts.append('</thead>')
                    html_parts.append('<tbody>')
                    for row_data in table_data:
                        html_parts.append('<tr>')
                        for cell_text in row_data:
                            html_parts.append(f'<td style="border: 1px solid #ddd; padding: 8px;">{self._aw_format_cell_text(cell_text, product_name, product_url)}</td>')
                        html_parts.append('</tr>')
                    html_parts.append('</tbody>')
                    html_parts.append('</table>')
            
            # å¦‚æœæœ€åè¿˜åœ¨åˆ—è¡¨ä¸­ï¼Œå…³é—­æ ‡ç­¾
            if in_list:
                html_parts.append('</ul>')
            if in_ordered_list:
                html_parts.append('</ol>')
        else:
            # å¦‚æœæ²¡æœ‰å†…å®¹ï¼Œæç¤ºé”™è¯¯å¹¶ç»™å‡ºä½¿ç”¨è¯´æ˜
            html_parts.append('<h2>âš ï¸ é”™è¯¯ï¼šæ–‡ç« å†…å®¹ä¸ºç©º</h2>')
            html_parts.append('<p>æ­¤å·¥å…·éœ€è¦ article_content å‚æ•°æä¾›å®Œæ•´çš„æ–‡ç« å†…å®¹ã€‚</p>')
            html_parts.append('<h3>æ­£ç¡®çš„ä½¿ç”¨æµç¨‹ï¼š</h3>')
            html_parts.append('<ol>')
            html_parts.append('<li>ç¬¬ä¸€æ­¥ï¼šå…ˆç”Ÿæˆå®Œæ•´çš„æ–‡ç« å†…å®¹ï¼ˆMarkdownæ ¼å¼ï¼‰</li>')
            html_parts.append('<li>ç¬¬äºŒæ­¥ï¼šè°ƒç”¨ write_complete_article å·¥å…·ï¼Œå°†ç”Ÿæˆçš„å†…å®¹ä½œä¸º article_content å‚æ•°ä¼ å…¥</li>')
            html_parts.append('</ol>')
            html_parts.append('<p><strong>è¯·ä¸è¦åœ¨æœªç”Ÿæˆæ–‡ç« å†…å®¹çš„æƒ…å†µä¸‹è°ƒç”¨æ­¤å·¥å…·ã€‚</strong></p>')
        
        # åœ¨åˆé€‚çš„ä½ç½®æ·»åŠ AIç”Ÿæˆçš„å›¾ç‰‡
        # å°†å›¾ç‰‡æ’å…¥åˆ°æ–‡ç« çš„åˆé€‚ä½ç½®ï¼ˆåœ¨æ­£æ–‡å†…å®¹ä¹‹åï¼ŒCTAä¹‹å‰ï¼‰
        if image_urls:
            image_list = [url.strip() for url in image_urls] if isinstance(image_urls, (list, tuple)) else ([url.strip() for url in image_urls.split(',')] if image_urls else [])
            image_list = [url for url in image_list if url]  # ç§»é™¤ç©ºURL
            
            if image_list:
                # åœ¨æ­£æ–‡å†…å®¹ä¹‹åæ’å…¥å›¾ç‰‡ï¼ˆåœ¨å†…å®¹è½¬æ¢å®Œæˆåï¼‰
                # ç”Ÿæˆåˆé€‚çš„altæ–‡æœ¬
                alt_keyword = keyword if keyword else product_name
                for idx, img_url in enumerate(image_list[:3], 1):  # æœ€å¤š3å¼ å›¾ç‰‡
                    alt_text = f"{alt_keyword} - AI generated image {idx}" if alt_keyword else f"AI generated image {idx}"
                    html_parts.append(f'<p style="text-align: center; margin: 30px 0;"><img src="{img_url}" alt="{alt_text}" style="max-width: 100%; height: auto; border-radius: 8px;" /></p>')
                    html_parts.append('<p style="text-align: center; font-style: italic; color: #666; font-size: 0.9em; margin-top: -15px; margin-bottom: 30px;">AI Generated Image</p>')
        
        # CTA
        html_parts.append('')
        html_parts.append(f'<p><strong>Ready to experience {product_name}?</strong> <a href="{product_url}">Visit {product_url}</a> to get started today.</p>')
        
        return '\n'.join(html_parts)
    
    def _aw_publish_to_wordpress(self, title: str, article_content: str, product_name: str,
                                 product_url: str, product_description: str,
                                 search_intent_analysis: str, image_urls: list,
                                 categories: str, tags: str, status: str, language: str, keyword: str = "") -> dict:
        """å‘å¸ƒæ–‡ç« åˆ°WordPress"""
        token = self.valves.WP_ACCESS_TOKEN.strip()
        site_id = self.valves.WP_SITE_ID.strip()
        
        if not token:
            return {"success": False, "error": "æœªé…ç½® WordPress Access Tokenï¼Œè¯·åœ¨å·¥å…·è®¾ç½®ä¸­é…ç½®"}
        
        if not site_id:
            return {"success": False, "error": "æœªé…ç½® WordPress Site IDï¼Œè¯·åœ¨å·¥å…·è®¾ç½®ä¸­é…ç½®"}
        
        try:
            # è½¬æ¢ä¸ºHTMLï¼ˆä¼ å…¥ç”Ÿæˆçš„AIå›¾ç‰‡URLåˆ—è¡¨ï¼‰
            html_image_urls = image_urls if isinstance(image_urls, (list, tuple)) else ([url.strip() for url in image_urls.split(',')] if isinstance(image_urls, str) and image_urls else [])
            html_content = self._aw_convert_to_html(
                article_content, product_name, product_url, product_description,
                search_intent_analysis, html_image_urls, language, keyword=keyword
            )
            
            # æ„å»ºAPIè¯·æ±‚
            api_base = self.valves.WP_API_BASE.rstrip('/')
            url = f"{api_base}/sites/{site_id}/posts/new"
            
            headers = {
                "Authorization": f"Bearer {token}",
                "Content-Type": "application/json",
                "User-Agent": "OpenWebUI-Article-Writer/2.0"
            }
            
            post_data = {
                "title": title,
                "content": html_content,
                "status": status
            }
            
            if categories:
                post_data["categories"] = categories
            if tags:
                post_data["tags"] = tags
            
            # å‘é€è¯·æ±‚
            response = requests.post(url, json=post_data, headers=headers, timeout=30, verify=False)
            
            if response.status_code == 200:
                result = response.json()
                return {
                    "success": True,
                    "post_id": result.get("ID"),
                    "url": result.get("URL"),
                    "data": result
                }
            else:
                error_msg = response.text
                try:
                    error_json = response.json()
                    error_msg = error_json.get("message", error_msg)
                except:
                    pass
                return {"success": False, "error": f"WordPress API é”™è¯¯ ({response.status_code}): {error_msg}"}
        
        except requests.exceptions.RequestException as e:
            return {"success": False, "error": f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}"}
        except Exception as e:
            return {"success": False, "error": f"å‘å¸ƒå¤±è´¥: {str(e)}"}

    def _aw_add_hyperlink(self, paragraph, text: str, url: str):
        """åœ¨Wordæ–‡æ¡£ä¸­æ·»åŠ è¶…é“¾æ¥"""
        # è·å–æ®µè½çš„éƒ¨åˆ†ï¼ˆpartï¼‰
        part = paragraph.part
        
        # åˆ›å»ºè¶…é“¾æ¥å…³ç³»
        r_id = part.relate_to(url, 'http://schemas.openxmlformats.org/officeDocument/2006/relationships/hyperlink', is_external=True)
        
        # åˆ›å»ºè¶…é“¾æ¥å…ƒç´ 
        hyperlink = OxmlElement('w:hyperlink')
        hyperlink.set(qn('r:id'), r_id)
        
        # åˆ›å»ºè¿è¡Œå…ƒç´ 
        new_run = OxmlElement('w:r')
        rPr = OxmlElement('w:rPr')
        
        # è®¾ç½®é“¾æ¥æ ·å¼ï¼ˆè“è‰²ã€ä¸‹åˆ’çº¿ï¼‰
        color = OxmlElement('w:color')
        color.set(qn('w:val'), '0000FF')
        rPr.append(color)
        
        u = OxmlElement('w:u')
        u.set(qn('w:val'), 'single')
        rPr.append(u)
        
        new_run.append(rPr)
        
        # æ·»åŠ æ–‡æœ¬
        text_element = OxmlElement('w:t')
        text_element.text = text
        new_run.append(text_element)
        
        hyperlink.append(new_run)
        paragraph._p.append(hyperlink)

    def write_complete_article(
        self,
        keyword: str,
        product_name: str,
        product_url: str,
        product_description: str,
        article_content: str = "",
        search_intent_analysis: str = "",
        image_urls: str = "",
        word_count: int = 2000,
        language: str = "en",
        publish_to_wordpress: bool = False,
        wp_categories: str = "",
        wp_tags: str = "",
        wp_status: str = "publish"
    ) -> str:
        """
        ã€å®Œæ•´æ–‡ç« ç”Ÿæˆå·¥å…·ã€‘åŸºäºæœç´¢æ„å›¾åˆ†æç”Ÿæˆå®Œæ•´çš„GEOä¼˜åŒ–æ–‡ç« ï¼ˆå€’é‡‘å­—å¡”ç»“æ„ï¼‰ï¼Œæ”¯æŒç”ŸæˆWordæ–‡æ¡£å’Œ/æˆ–ç›´æ¥å‘å¸ƒåˆ°WordPress
        
        âš ï¸ é‡è¦æç¤ºï¼šæ­¤å·¥å…·è¦æ±‚ article_content å‚æ•°å¿…é¡»æä¾›å®Œæ•´çš„æ–‡ç« å†…å®¹ã€‚
        
        ä½¿ç”¨æµç¨‹ï¼š
        1. ç¬¬ä¸€æ­¥ï¼šLLM å…ˆç”Ÿæˆå®Œæ•´çš„æ–‡ç« å†…å®¹ï¼ˆMarkdownæ ¼å¼ï¼ŒåŒ…å«æ ‡é¢˜ã€æ®µè½ã€åˆ—è¡¨ç­‰ï¼‰
        2. ç¬¬äºŒæ­¥ï¼šå°†ç”Ÿæˆçš„æ–‡ç« å†…å®¹ä½œä¸º article_content å‚æ•°ä¼ å…¥æ­¤å·¥å…·
        
        âŒ é”™è¯¯ç”¨æ³•ï¼šç›´æ¥è°ƒç”¨å·¥å…·è€Œä¸æä¾› article_contentï¼ˆä¼šç”Ÿæˆé”™è¯¯æç¤ºï¼‰
        âœ… æ­£ç¡®ç”¨æ³•ï¼šå…ˆç”Ÿæˆå†…å®¹ï¼Œå†è°ƒç”¨å·¥å…·æ ¼å¼åŒ–å¹¶å‘å¸ƒ
        
        å½“ç”¨æˆ·è¯´ä»¥ä¸‹å†…å®¹æ—¶è°ƒç”¨æ­¤å·¥å…·ï¼š
        - "åˆ†ææœç´¢æ„å›¾å¹¶å†™æ–‡ç« " â†’ å…ˆè®©LLMç”Ÿæˆæ–‡ç« å†…å®¹ï¼Œå†è°ƒç”¨æ­¤å·¥å…·
        - "å†™ä¸€ç¯‡å®Œæ•´çš„æ–‡ç« ï¼Œæ¨èXXX" â†’ å…ˆè®©LLMç”Ÿæˆæ–‡ç« å†…å®¹ï¼Œå†è°ƒç”¨æ­¤å·¥å…·
        - "åŸºäºæœç´¢å…³é”®è¯ç”Ÿæˆå®Œæ•´æ–‡ç« " â†’ å…ˆè®©LLMç”Ÿæˆæ–‡ç« å†…å®¹ï¼Œå†è°ƒç”¨æ­¤å·¥å…·
        - "å€’é‡‘å­—å¡”ç»“æ„æ–‡ç« " â†’ å…ˆè®©LLMç”Ÿæˆæ–‡ç« å†…å®¹ï¼Œå†è°ƒç”¨æ­¤å·¥å…·
        - "å†™æ–‡ç« å¹¶å‘å¸ƒåˆ°WordPress"ï¼ˆè®¾ç½® publish_to_wordpress=Trueï¼‰â†’ å…ˆè®©LLMç”Ÿæˆæ–‡ç« å†…å®¹ï¼Œå†è°ƒç”¨æ­¤å·¥å…·
        
        :param keyword: æœç´¢å…³é”®è¯
        :param product_name: äº§å“åç§°
        :param product_url: äº§å“å®˜ç½‘URL
        :param product_description: äº§å“æè¿°
        :param article_content: å®Œæ•´æ–‡ç« å†…å®¹ï¼ˆHTML/Markdownæ ¼å¼ï¼Œã€å¿…å¡«ã€‘LLMéœ€è¦å…ˆç”Ÿæˆå®Œæ•´æ–‡ç« å†…å®¹å†è°ƒç”¨æ­¤å·¥å…·ï¼Œä¸èƒ½ä¸ºç©ºï¼‰
        :param search_intent_analysis: æœç´¢æ„å›¾åˆ†æç»“æœï¼ˆå¯é€‰ï¼‰
        :param image_urls: å›¾ç‰‡URLåˆ—è¡¨ï¼Œé€—å·åˆ†éš”ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›ï¼Œå°†è‡ªåŠ¨ç”ŸæˆSEOå‹å¥½çš„AIå›¾ç‰‡URLï¼‰
        :param word_count: ç›®æ ‡å­—æ•°
        :param language: æ–‡ç« è¯­è¨€ (en=è‹±æ–‡, zh=ä¸­æ–‡)
        :param publish_to_wordpress: æ˜¯å¦å‘å¸ƒåˆ°WordPressï¼ˆé»˜è®¤Falseï¼Œä»…ç”ŸæˆWordæ–‡æ¡£ï¼‰
        :param wp_categories: WordPressåˆ†ç±»ï¼Œé€—å·åˆ†éš”ï¼ˆå¯é€‰ï¼‰
        :param wp_tags: WordPressæ ‡ç­¾ï¼Œé€—å·åˆ†éš”ï¼ˆå¯é€‰ï¼‰
        :param wp_status: WordPresså‘å¸ƒçŠ¶æ€ - "publish"(ç«‹å³å‘å¸ƒ), "draft"(è‰ç¨¿ï¼Œé»˜è®¤)
        :return: Wordæ–‡æ¡£è·¯å¾„å’Œç”Ÿæˆç»“æœï¼ˆå¦‚æœå‘å¸ƒåˆ°WordPressï¼ŒåŒ…å«WordPress URLï¼‰
        """
        output_path = self._aw_ensure_output_dir()
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_keyword = keyword.replace(' ', '_').replace('/', '_')[:50]
        filename = f"article_{safe_keyword}_{timestamp}.docx"
        filepath = os.path.join(output_path, filename)
        
        # åˆ›å»ºWordæ–‡æ¡£
        doc = Document()
        
        # è®¾ç½®é»˜è®¤å­—ä½“
        style = doc.styles['Normal']
        font = style.font
        font.name = 'Calibri'
        font.size = Pt(11)
        
        # ===== å€’é‡‘å­—å¡”ç»“æ„ï¼šæ ¸å¿ƒæ¨èï¼ˆé¡¶éƒ¨ï¼‰=====
        # ä¸»æ ‡é¢˜
        title_heading = doc.add_heading(keyword.title() if language == 'en' else keyword, 0)
        
        # TL;DR éƒ¨åˆ†ï¼ˆæœ€é‡è¦çš„å†…å®¹åœ¨å¼€å¤´ï¼‰
        tldr_para = doc.add_paragraph()
        tldr_run = tldr_para.add_run('TL;DR: ')
        tldr_run.bold = True
        tldr_run.font.size = Pt(12)
        
        # å¦‚æœæä¾›äº†æ–‡ç« å†…å®¹ï¼Œæå–TL;DRï¼›å¦åˆ™ç”Ÿæˆå ä½ç¬¦
        if article_content:
            # å°è¯•ä»å†…å®¹ä¸­æå–TL;DR
            tldr_match = re.search(r'(?i)tl;?dr[:\s]+(.*?)(?:\n\n|\n##|\n#|$)', article_content, re.DOTALL)
            if tldr_match:
                tldr_text = tldr_match.group(1).strip()
                # æ¸…ç†Markdownæ ¼å¼
                tldr_text = re.sub(r'\*\*(.*?)\*\*', r'\1', tldr_text)
                tldr_para.add_run(tldr_text[:300])
            else:
                tldr_para.add_run(f'{product_name} is the top recommendation for "{keyword}". Based on comprehensive analysis, it offers [key advantage 1], [key advantage 2], making it the ideal choice for [target users].')
        else:
            tldr_para.add_run(f'{product_name} stands out as the premier solution for "{keyword}". Our analysis reveals [key benefits]. Visit {product_url} to get started.')
        
        doc.add_paragraph()  # ç©ºè¡Œ
        
        # ===== æœç´¢æ„å›¾åˆ†æï¼ˆå¦‚æœæä¾›ï¼‰=====
        if search_intent_analysis:
            intent_para = doc.add_paragraph()
            intent_run = intent_para.add_run('Search Intent: ')
            intent_run.bold = True
            intent_para.add_run(search_intent_analysis)
            doc.add_paragraph()  # ç©ºè¡Œ
        
        # ===== å¤„ç†æ–‡ç« å†…å®¹ =====
        if article_content:
            # æ¸…ç†HTML/Markdownæ ‡ç­¾å¹¶è½¬æ¢ä¸ºWordæ ¼å¼
            content = article_content.strip()
            
            # ç§»é™¤TL;DRéƒ¨åˆ†ï¼ˆå·²åœ¨å‰é¢æ·»åŠ ï¼‰
            content = re.sub(r'(?i)^#+\s*tl;?dr[:\s]*.*?(?=\n\n|\n##|\n#|$)', '', content, flags=re.DOTALL | re.MULTILINE)
            
            # é¢„å¤„ç†ï¼šå°†Unicodeè½¬ä¹‰å­—ç¬¦è½¬æ¢ä¸ºå®é™…å­—ç¬¦
            def decode_unicode_escapes(text):
                """å°†Unicodeè½¬ä¹‰å­—ç¬¦è½¬æ¢ä¸ºå®é™…å­—ç¬¦"""
                if not text:
                    return text
                try:
                    # å¤„ç†å­—ç¬¦ä¸²ä¸­çš„Unicodeè½¬ä¹‰åºåˆ—ï¼ˆ\uXXXXæ ¼å¼ï¼‰
                    # é¦–å…ˆå¤„ç†å¸¸è§çš„Unicodeè½¬ä¹‰å­—ç¬¦ï¼ˆç›´æ¥çš„æ›¿æ¢ï¼‰
                    text = str(text).replace('\\u2b50', 'â­')  # æ˜Ÿå·
                    text = text.replace('\\u2605', 'â˜…')  # å®å¿ƒæ˜Ÿ
                    text = text.replace('\\u2606', 'â˜†')  # ç©ºå¿ƒæ˜Ÿ
                    
                    # é€šç”¨Unicodeè½¬ä¹‰å¤„ç†ï¼šåŒ¹é… \u åè·Ÿ4ä¸ªåå…­è¿›åˆ¶æ•°å­—
                    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…æ‰€æœ‰ \uXXXX æ ¼å¼çš„è½¬ä¹‰åºåˆ—
                    def replace_unicode(match):
                        try:
                            code_point = int(match.group(1), 16)
                            return chr(code_point)
                        except:
                            return match.group(0)  # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œè¿”å›åŸå§‹å­—ç¬¦ä¸²
                    
                    text = re.sub(r'\\u([0-9a-fA-F]{4})', replace_unicode, text)
                    
                    # å¤„ç†å…¶ä»–å¸¸è§è½¬ä¹‰
                    text = text.replace('\\n', '\n').replace('\\t', '\t').replace('\\r', '\r')
                    text = text.replace('\\"', '"').replace("\\'", "'")
                except Exception as e:
                    # å¦‚æœå¤„ç†å¤±è´¥ï¼Œè¿”å›åŸå§‹æ–‡æœ¬
                    pass
                return text
            
            # åˆ†å‰²ä¸ºæ®µè½å’Œæ ‡é¢˜
            lines = content.split('\n')
            current_paragraph = None
            in_table = False
            table_data = []
            table_headers = None
            
            for line in lines:
                line = line.strip()
                if not line:
                    # å¦‚æœä¹‹å‰åœ¨è¡¨æ ¼ä¸­ï¼Œç»“æŸè¡¨æ ¼å¤„ç†
                    if in_table and table_data:
                        # åˆ›å»ºWordè¡¨æ ¼
                        if table_headers:
                            table = doc.add_table(rows=1, cols=len(table_headers))
                            table.style = 'Light Grid Accent 1'
                            
                            # è®¾ç½®è¡¨å¤´ï¼ˆheaderå·²ç»åœ¨ä¹‹å‰å¤„ç†è¿‡äº†ï¼Œä½†å†æ¬¡ç¡®ä¿ï¼‰
                            header_cells = table.rows[0].cells
                            for i, header in enumerate(table_headers):
                                header_cells[i].text = header  # å·²ç»åœ¨å‰é¢decodeè¿‡äº†
                                header_cells[i].paragraphs[0].runs[0].font.bold = True
                            
                            # æ·»åŠ æ•°æ®è¡Œï¼ˆcell_textå·²ç»åœ¨å‰é¢decodeè¿‡äº†ï¼‰
                            for row_data in table_data:
                                row_cells = table.add_row().cells
                                for i, cell_text in enumerate(row_data):
                                    if i < len(header_cells):
                                        row_cells[i].text = cell_text  # å·²ç»åœ¨å‰é¢decodeè¿‡äº†
                            
                            # é‡ç½®è¡¨æ ¼çŠ¶æ€
                            in_table = False
                            table_data = []
                            table_headers = None
                    elif current_paragraph:
                        doc.add_paragraph()  # ç©ºè¡Œ
                    current_paragraph = None
                    continue
                
                # æ£€æµ‹æ ‡é¢˜ (Markdownæ ¼å¼)
                h1_match = re.match(r'^#\s+(.+)$', line)
                h2_match = re.match(r'^##\s+(.+)$', line)
                h3_match = re.match(r'^###\s+(.+)$', line)
                
                if h1_match:
                    # è·³è¿‡ä¸»æ ‡é¢˜ï¼ˆå·²åœ¨å‰é¢æ·»åŠ ï¼‰
                    continue
                elif h2_match:
                    # å¦‚æœåœ¨è¡¨æ ¼ä¸­ï¼Œå…ˆç»“æŸè¡¨æ ¼
                    if in_table and table_data:
                        if table_headers:
                            table = doc.add_table(rows=1, cols=len(table_headers))
                            table.style = 'Light Grid Accent 1'
                            header_cells = table.rows[0].cells
                            for i, header in enumerate(table_headers):
                                header_cells[i].text = header  # å·²ç»åœ¨å‰é¢decodeè¿‡äº†
                                header_cells[i].paragraphs[0].runs[0].font.bold = True
                            for row_data in table_data:
                                row_cells = table.add_row().cells
                                for i, cell_text in enumerate(row_data):
                                    if i < len(header_cells):
                                        row_cells[i].text = cell_text  # å·²ç»åœ¨å‰é¢decodeè¿‡äº†
                    in_table = False
                    table_data = []
                    table_headers = None
                    doc.add_heading(h2_match.group(1), level=1)
                    current_paragraph = None
                elif h3_match:
                    # å¦‚æœåœ¨è¡¨æ ¼ä¸­ï¼Œå…ˆç»“æŸè¡¨æ ¼
                    if in_table and table_data:
                        if table_headers:
                            table = doc.add_table(rows=1, cols=len(table_headers))
                            table.style = 'Light Grid Accent 1'
                            header_cells = table.rows[0].cells
                            for i, header in enumerate(table_headers):
                                header_cells[i].text = header  # å·²ç»åœ¨å‰é¢decodeè¿‡äº†
                                header_cells[i].paragraphs[0].runs[0].font.bold = True
                            for row_data in table_data:
                                row_cells = table.add_row().cells
                                for i, cell_text in enumerate(row_data):
                                    if i < len(header_cells):
                                        row_cells[i].text = cell_text  # å·²ç»åœ¨å‰é¢decodeè¿‡äº†
                    in_table = False
                    table_data = []
                    table_headers = None
                    doc.add_heading(h3_match.group(1), level=2)
                    current_paragraph = None
                elif line.startswith('|') and '|' in line[1:]:
                    # è¡¨æ ¼è¡Œå¤„ç†
                    cells = [decode_unicode_escapes(cell.strip()) for cell in line.split('|')[1:-1]]
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯åˆ†éš”è¡Œï¼ˆå…¨éƒ¨æ˜¯---æˆ–ç±»ä¼¼çš„ï¼‰
                    if all(re.match(r'^[\s\-:]+$', c) for c in cells):
                        continue  # è·³è¿‡åˆ†éš”è¡Œ
                    
                    if not in_table:
                        # ç¬¬ä¸€è¡Œæ˜¯è¡¨å¤´
                        in_table = True
                        table_headers = cells
                        table_data = []
                    else:
                        # æ•°æ®è¡Œ
                        table_data.append(cells)
                elif line.startswith('- ') or line.startswith('* '):
                    # åˆ—è¡¨é¡¹
                    para = doc.add_paragraph(style='List Bullet')
                    para.add_run(decode_unicode_escapes(line[2:]))
                elif line.startswith('**') and line.endswith('**'):
                    # ç²—ä½“æ®µè½
                    para = doc.add_paragraph()
                    para.add_run(decode_unicode_escapes(line.replace('**', ''))).bold = True
                elif not in_table:
                    # æ™®é€šæ®µè½ï¼ˆä¸åœ¨è¡¨æ ¼ä¸­ï¼‰
                    para = doc.add_paragraph()
                    text = decode_unicode_escapes(line)
                    
                    # å¤„ç†å†…è”æ ¼å¼å’Œé“¾æ¥
                    # ç²—ä½“
                    text = re.sub(r'\*\*(.*?)\*\*', r'\1', text)
                    # é“¾æ¥ [text](url)
                    link_pattern = r'\[([^\]]+)\]\(([^\)]+)\)'
                    links = re.findall(link_pattern, text)
                    if links:
                        # åˆ†å‰²æ–‡æœ¬å¹¶æ’å…¥é“¾æ¥
                        last_pos = 0
                        for link_text, link_url in links:
                            # æ‰¾åˆ°é“¾æ¥åœ¨æ–‡æœ¬ä¸­çš„ä½ç½®
                            link_start = text.find(f'[{link_text}]({link_url})', last_pos)
                            if link_start >= 0:
                                # æ·»åŠ é“¾æ¥å‰çš„æ–‡æœ¬
                                if link_start > last_pos:
                                    para.add_run(text[last_pos:link_start])
                                # æ·»åŠ è¶…é“¾æ¥
                                self._aw_add_hyperlink(para, link_text, link_url)
                                last_pos = link_start + len(f'[{link_text}]({link_url})')
                        # æ·»åŠ å‰©ä½™æ–‡æœ¬
                        if last_pos < len(text):
                            para.add_run(text[last_pos:])
                    else:
                        # æ£€æŸ¥æ˜¯å¦åŒ…å«äº§å“URLæˆ–äº§å“åï¼Œè‡ªåŠ¨æ·»åŠ é“¾æ¥
                        if product_name in text:
                            # åˆ†å‰²æ–‡æœ¬å¹¶åœ¨äº§å“åå¤„æ’å…¥é“¾æ¥
                            parts = text.split(product_name)
                            for i, part in enumerate(parts):
                                if i > 0:
                                    # æ·»åŠ äº§å“åä½œä¸ºè¶…é“¾æ¥
                                    self._aw_add_hyperlink(para, product_name, product_url)
                                if part:
                                    para.add_run(part)
                        elif product_url in text:
                            # å¦‚æœæ–‡æœ¬ä¸­åŒ…å«URLï¼Œæå–å¹¶æ·»åŠ ä¸ºé“¾æ¥
                            para.add_run(text)
                        else:
                            para.add_run(text)
                    
                    current_paragraph = para
            
            # å¦‚æœå†…å®¹ç»“æŸæ—¶è¿˜åœ¨è¡¨æ ¼ä¸­ï¼Œç»“æŸè¡¨æ ¼
            if in_table and table_data:
                if table_headers:
                    table = doc.add_table(rows=1, cols=len(table_headers))
                    table.style = 'Light Grid Accent 1'
                    header_cells = table.rows[0].cells
                    for i, header in enumerate(table_headers):
                        header_cells[i].text = header  # å·²ç»åœ¨å‰é¢decodeè¿‡äº†
                        header_cells[i].paragraphs[0].runs[0].font.bold = True
                    for row_data in table_data:
                        row_cells = table.add_row().cells
                        for i, cell_text in enumerate(row_data):
                            if i < len(header_cells):
                                row_cells[i].text = cell_text  # å·²ç»åœ¨å‰é¢decodeè¿‡äº†
        else:
            # å¦‚æœæ²¡æœ‰æä¾›å†…å®¹ï¼Œæç¤ºé”™è¯¯å¹¶ç»™å‡ºä½¿ç”¨è¯´æ˜
            doc.add_heading("âš ï¸ é”™è¯¯ï¼šæ–‡ç« å†…å®¹ä¸ºç©º", level=1)
            doc.add_paragraph("æ­¤å·¥å…·éœ€è¦ article_content å‚æ•°æä¾›å®Œæ•´çš„æ–‡ç« å†…å®¹ã€‚")
            doc.add_paragraph("")
            doc.add_paragraph("æ­£ç¡®çš„ä½¿ç”¨æµç¨‹ï¼š")
            doc.add_paragraph("1. ç¬¬ä¸€æ­¥ï¼šå…ˆç”Ÿæˆå®Œæ•´çš„æ–‡ç« å†…å®¹ï¼ˆMarkdownæ ¼å¼ï¼‰", style='List Bullet')
            doc.add_paragraph("2. ç¬¬äºŒæ­¥ï¼šè°ƒç”¨ write_complete_article å·¥å…·ï¼Œå°†ç”Ÿæˆçš„å†…å®¹ä½œä¸º article_content å‚æ•°ä¼ å…¥", style='List Bullet')
            doc.add_paragraph("")
            doc.add_paragraph("è¯·ä¸è¦åœ¨æœªç”Ÿæˆæ–‡ç« å†…å®¹çš„æƒ…å†µä¸‹è°ƒç”¨æ­¤å·¥å…·ã€‚")
        
        # ===== ç”ŸæˆAIå›¾ç‰‡URLï¼ˆSEOå‹å¥½ï¼‰=====
        # ä¸å†çˆ¬å–URLå›¾ç‰‡ï¼Œè€Œæ˜¯ç”ŸæˆSEOå‹å¥½çš„AIå›¾ç‰‡URL
        all_image_urls = []
        
        # å¦‚æœæä¾›äº†å›¾ç‰‡URLåˆ—è¡¨ï¼Œä½¿ç”¨å®ƒä»¬
        if image_urls:
            all_image_urls.extend([url.strip() for url in image_urls.split(',') if url.strip()])
        else:
            # ç”Ÿæˆ2-3å¼ AIå›¾ç‰‡URLï¼ˆåœ¨æ–‡ç« ä¸­åˆé€‚çš„ä½ç½®æ’å…¥ï¼‰
            num_images = min(3, max(2, word_count // 800))  # æ ¹æ®å­—æ•°å†³å®šå›¾ç‰‡æ•°é‡
            for i in range(1, num_images + 1):
                img_url = self._aw_generate_seo_image_url(keyword, image_index=i, base_domain=product_url)
                all_image_urls.append(img_url)
        
        # åœ¨Wordæ–‡æ¡£ä¸­æ·»åŠ å›¾ç‰‡å ä½ç¬¦è¯´æ˜ï¼ˆå®é™…å›¾ç‰‡ä¼šåœ¨HTMLè½¬æ¢æ—¶æ’å…¥ï¼‰
        if all_image_urls:
            doc.add_paragraph()  # ç©ºè¡Œ
            para = doc.add_paragraph()
            para.add_run('Images (AI Generated, SEO-friendly URLs):').bold = True
            for idx, img_url in enumerate(all_image_urls, 1):
                doc.add_paragraph(f'  Image {idx}: {img_url}', style='List Bullet')
        
        # ===== äº§å“æ¨èCTA =====
        doc.add_paragraph()  # ç©ºè¡Œ
        cta_para = doc.add_paragraph()
        cta_para.add_run('Ready to experience ').bold = True
        cta_para.add_run(product_name).bold = True
        cta_para.add_run('? ').bold = True
        self._aw_add_hyperlink(cta_para, 'Visit ' + product_url, product_url)
        cta_para.add_run(' to get started today.')
        
        # ä¿å­˜æ–‡æ¡£
        doc.save(filepath)
        
        # ===== å‘å¸ƒåˆ° WordPressï¼ˆå¦‚æœå¯ç”¨ï¼‰=====
        wp_result_text = ""
        wp_url = None
        if publish_to_wordpress:
            wp_result = self._aw_publish_to_wordpress(
                title=keyword.title() if language == 'en' else keyword,
                article_content=article_content,
                product_name=product_name,
                product_url=product_url,
                product_description=product_description,
                search_intent_analysis=search_intent_analysis,
                image_urls=all_image_urls,
                categories=wp_categories,
                tags=wp_tags,
                status=wp_status,
                language=language,
                keyword=keyword
            )
            if wp_result.get("success"):
                wp_url = wp_result.get("url")
                status_text = "å·²å‘å¸ƒ" if wp_status == "publish" else "å·²ä¿å­˜ä¸ºè‰ç¨¿"
                wp_result_text = f"\n\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nğŸš€ **WordPress {status_text}**\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nğŸ†” æ–‡ç« ID: {wp_result.get('post_id', 'N/A')}\nğŸ”— æ–‡ç« URL: {wp_url}\n"
            else:
                wp_result_text = f"\n\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nâŒ **WordPress å‘å¸ƒå¤±è´¥**\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\né”™è¯¯: {wp_result.get('error', 'æœªçŸ¥é”™è¯¯')}\n"
        
        # ç”Ÿæˆè¿”å›ä¿¡æ¯
        content_preview = article_content[:500] + '...' if article_content and len(article_content) > 500 else (article_content or 'Framework generated')
        
        result = f"""
ğŸ“ **å®Œæ•´æ–‡ç« ç”Ÿæˆå®Œæˆ**

ğŸ¯ æœç´¢å…³é”®è¯: {keyword}
ğŸ“¦ æ¨èäº§å“: {product_name}
ğŸ”— äº§å“é“¾æ¥: {product_url}
ğŸ“Š ç›®æ ‡å­—æ•°: {word_count} è¯
ğŸŒ è¯­è¨€: {'è‹±æ–‡' if language == 'en' else 'ä¸­æ–‡'}
ğŸ“ ç»“æ„: å€’é‡‘å­—å¡”ç»“æ„ï¼ˆæ ¸å¿ƒæ¨èâ†’æ”¯æ’‘è®ºæ®â†’è¯¦ç»†ä¿¡æ¯ï¼‰

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ’¾ **Wordæ–‡æ¡£å·²ä¿å­˜**
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
è·¯å¾„: {filepath}
{wp_result_text}
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“‹ **æ–‡ç« ç»“æ„**
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
1. TL;DR æ ¸å¿ƒæ¨èï¼ˆé¡¶éƒ¨ï¼‰
2. æœç´¢æ„å›¾åˆ†æ
3. æ­£æ–‡å†…å®¹ï¼ˆå€’é‡‘å­—å¡”ç»“æ„ï¼‰
   - æ ¸å¿ƒè®ºç‚¹
   - æ”¯æ’‘è®ºæ®
   - è¯¦ç»†ä¿¡æ¯
4. äº§å“æ¨èä¸é“¾æ¥
5. è¡ŒåŠ¨å·å¬ (CTA)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“„ **å†…å®¹é¢„è§ˆ**
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
{content_preview}

ğŸ’¡ **è¯´æ˜**:
- æ–‡ç« é‡‡ç”¨å€’é‡‘å­—å¡”ç»“æ„ï¼Œæ ¸å¿ƒæ¨èä½äºå¼€å¤´
- äº§å“æ¨èå·²è‡ªç„¶èå…¥æ­£æ–‡
- åŒ…å«äº§å“å®˜ç½‘è¶…é“¾æ¥
- å›¾ç‰‡å·²æ·»åŠ ï¼ˆå¦‚æœ‰æä¾›ï¼‰
- ä»…åŒ…å«æ­£æ–‡å†…å®¹ï¼Œæ— å…ƒä¿¡æ¯
"""
        return result

